{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734c9682-f3c2-41d5-a340-8b59bb5ae792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 29 00:42:33 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.27       Driver Version: 465.27       CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA Tesla V1...  Off  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   57C    P0    40W / 250W |  28496MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "719acfd3-c9fb-4e1a-ba11-4e2044c141b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /opt/conda/lib/python3.7/site-packages (0.42.1)\n",
      "Requirement already satisfied: pypinyin in /opt/conda/lib/python3.7/site-packages (0.42.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.9.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.5.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.1)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U jieba pypinyin transformers --proxy http://10.8.84.123:7890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154c9e75-f8de-4cde-9de9-3a186c3f69d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load train dataset size: 731254\n",
      "load test dataset size: 22617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 0: 100%|██████████| 22852/22852 [1:43:33<00:00,  3.68it/s, loss=0.16045 d_precision: 0.96587 d_recall: 0.95037  c_acc: 0.96735 ]\n",
      "test epoch 0: 100%|██████████| 707/707 [01:07<00:00, 10.51it/s, loss=0.14216 d_precision: 0.96874 d_recall: 0.94854  c_acc: 0.97066 ]\n",
      "train epoch 1: 100%|██████████| 22852/22852 [1:43:47<00:00,  3.67it/s, loss=0.11218 d_precision: 0.97244 d_recall: 0.95821  c_acc: 0.97474 ]\n",
      "test epoch 1: 100%|██████████| 707/707 [01:07<00:00, 10.52it/s, loss=0.098858 d_precision: 0.97187 d_recall: 0.95723  c_acc:  0.9772 ]\n",
      "train epoch 2: 100%|██████████| 22852/22852 [1:44:06<00:00,  3.66it/s, loss=0.093734 d_precision: 0.97438 d_recall: 0.96136  c_acc: 0.97742 ]\n",
      "test epoch 2: 100%|██████████| 707/707 [01:07<00:00, 10.52it/s, loss=0.082903 d_precision: 0.97502 d_recall:  0.9605  c_acc: 0.97973 ]\n",
      "train epoch 3: 100%|██████████| 22852/22852 [1:43:23<00:00,  3.68it/s, loss=0.085192 d_precision: 0.97589 d_recall: 0.96353  c_acc: 0.97876 ]\n",
      "test epoch 3: 100%|██████████| 707/707 [01:07<00:00, 10.53it/s, loss=0.075288 d_precision: 0.97518 d_recall: 0.96401  c_acc: 0.98095 ]\n",
      "train epoch 4: 100%|██████████| 22852/22852 [1:43:20<00:00,  3.69it/s, loss=0.080454 d_precision: 0.97887 d_recall: 0.96497  c_acc: 0.97976 ]\n",
      "test epoch 4: 100%|██████████| 707/707 [01:07<00:00, 10.52it/s, loss=0.070471 d_precision:  0.9778 d_recall: 0.96428  c_acc: 0.98173 ]\n",
      "train epoch 5: 100%|██████████| 22852/22852 [1:43:31<00:00,  3.68it/s, loss=0.075973 d_precision: 0.98092 d_recall: 0.96856  c_acc:  0.9802 ]\n",
      "test epoch 5: 100%|██████████| 707/707 [01:07<00:00, 10.52it/s, loss=0.067391 d_precision: 0.97592 d_recall: 0.96868  c_acc: 0.98222 ]\n",
      "train epoch 6: 100%|██████████| 22852/22852 [1:43:37<00:00,  3.68it/s, loss=0.07344 d_precision: 0.98073 d_recall: 0.96894  c_acc: 0.98038 ] \n",
      "test epoch 6: 100%|██████████| 707/707 [01:07<00:00, 10.53it/s, loss=0.064956 d_precision: 0.97875 d_recall: 0.96779  c_acc: 0.98263 ]\n",
      "train epoch 7: 100%|██████████| 22852/22852 [1:43:26<00:00,  3.68it/s, loss=0.069105 d_precision: 0.98303 d_recall: 0.97165  c_acc: 0.98119 ]\n",
      "test epoch 7: 100%|██████████| 707/707 [01:07<00:00, 10.52it/s, loss=0.062457 d_precision: 0.97862 d_recall:  0.9691  c_acc: 0.98305 ]\n",
      "train epoch 8: 100%|██████████| 22852/22852 [1:43:23<00:00,  3.68it/s, loss=0.066138 d_precision: 0.98345 d_recall: 0.97279  c_acc: 0.98146 ]\n",
      "test epoch 8: 100%|██████████| 707/707 [01:07<00:00, 10.52it/s, loss=0.061311 d_precision: 0.98142 d_recall: 0.96717  c_acc: 0.98321 ]\n",
      "train epoch 9: 100%|██████████| 22852/22852 [1:43:22<00:00,  3.68it/s, loss=0.064587 d_precision:  0.9838 d_recall: 0.97211  c_acc: 0.98203 ]\n",
      "test epoch 9: 100%|██████████| 707/707 [01:07<00:00, 10.54it/s, loss=0.06009 d_precision: 0.98182 d_recall: 0.96849  c_acc: 0.98341 ] \n",
      "train epoch 10: 100%|██████████| 22852/22852 [1:43:50<00:00,  3.67it/s, loss=0.06307 d_precision: 0.98545 d_recall: 0.97472  c_acc: 0.98238 ] \n",
      "test epoch 10: 100%|██████████| 707/707 [01:07<00:00, 10.51it/s, loss=0.058305 d_precision: 0.98038 d_recall: 0.97147  c_acc:  0.9837 ]\n",
      "train epoch 11: 100%|██████████| 22852/22852 [1:43:36<00:00,  3.68it/s, loss=0.062454 d_precision: 0.98573 d_recall: 0.97475  c_acc:  0.9823 ]\n",
      "test epoch 11: 100%|██████████| 707/707 [01:06<00:00, 10.55it/s, loss=0.05772 d_precision: 0.98036 d_recall: 0.97198  c_acc: 0.98385 ] \n",
      "train epoch 12: 100%|██████████| 22852/22852 [1:43:22<00:00,  3.68it/s, loss=0.060829 d_precision: 0.98469 d_recall: 0.97403  c_acc: 0.98266 ]\n",
      "test epoch 12: 100%|██████████| 707/707 [01:07<00:00, 10.53it/s, loss=0.056844 d_precision: 0.97947 d_recall: 0.97403  c_acc: 0.98398 ]\n",
      "train epoch 13: 100%|██████████| 22852/22852 [1:43:26<00:00,  3.68it/s, loss=0.058432 d_precision: 0.98559 d_recall: 0.97629  c_acc: 0.98299 ]\n",
      "test epoch 13: 100%|██████████| 707/707 [01:06<00:00, 10.58it/s, loss=0.05599 d_precision: 0.97873 d_recall: 0.97479  c_acc: 0.98415 ] \n",
      "train epoch 14: 100%|██████████| 22852/22852 [1:43:14<00:00,  3.69it/s, loss=0.058694 d_precision: 0.98628 d_recall: 0.97632  c_acc:  0.9832 ]\n",
      "test epoch 14: 100%|██████████| 707/707 [01:07<00:00, 10.52it/s, loss=0.055147 d_precision: 0.97906 d_recall: 0.97572  c_acc: 0.98427 ]\n",
      "train epoch 15: 100%|██████████| 22852/22852 [1:43:50<00:00,  3.67it/s, loss=0.05989 d_precision: 0.98587 d_recall: 0.97563  c_acc:  0.9829 ] \n",
      "test epoch 15: 100%|██████████| 707/707 [01:07<00:00, 10.50it/s, loss=0.055028 d_precision: 0.97924 d_recall: 0.97568  c_acc:  0.9843 ]\n",
      "train epoch 16: 100%|██████████| 22852/22852 [2:00:57<00:00,  3.15it/s, loss=0.057119 d_precision: 0.98605 d_recall:  0.9762  c_acc: 0.98346 ]  \n",
      "test epoch 16: 100%|██████████| 707/707 [01:07<00:00, 10.52it/s, loss=0.053619 d_precision: 0.97977 d_recall: 0.97678  c_acc: 0.98452 ]\n",
      "train epoch 17: 100%|██████████| 22852/22852 [1:43:31<00:00,  3.68it/s, loss=0.056078 d_precision: 0.98776 d_recall: 0.97699  c_acc:  0.9833 ]\n",
      "test epoch 17: 100%|██████████| 707/707 [01:07<00:00, 10.50it/s, loss=0.053399 d_precision: 0.98126 d_recall: 0.97479  c_acc: 0.98456 ]\n",
      "train epoch 18: 100%|██████████| 22852/22852 [1:43:22<00:00,  3.68it/s, loss=0.057386 d_precision: 0.98842 d_recall: 0.97626  c_acc: 0.98314 ]\n",
      "test epoch 18: 100%|██████████| 707/707 [01:07<00:00, 10.50it/s, loss=0.052741 d_precision: 0.98007 d_recall: 0.97682  c_acc: 0.98461 ]\n",
      "train epoch 19: 100%|██████████| 22852/22852 [1:43:25<00:00,  3.68it/s, loss=0.055198 d_precision:  0.9883 d_recall: 0.97849  c_acc:  0.9833 ]\n",
      "test epoch 19: 100%|██████████| 707/707 [01:07<00:00, 10.49it/s, loss=0.052731 d_precision: 0.98115 d_recall: 0.97602  c_acc: 0.98467 ]\n",
      "train epoch 20: 100%|██████████| 22852/22852 [1:43:23<00:00,  3.68it/s, loss=0.053603 d_precision: 0.98847 d_recall: 0.97928  c_acc: 0.98382 ]\n",
      "test epoch 20: 100%|██████████| 707/707 [01:07<00:00, 10.47it/s, loss=0.052339 d_precision: 0.98223 d_recall: 0.97505  c_acc: 0.98477 ]\n",
      "train epoch 21: 100%|██████████| 22852/22852 [1:43:24<00:00,  3.68it/s, loss=0.054438 d_precision: 0.98847 d_recall: 0.97888  c_acc: 0.98349 ]\n",
      "test epoch 21: 100%|██████████| 707/707 [01:07<00:00, 10.47it/s, loss=0.051993 d_precision: 0.97989 d_recall: 0.97785  c_acc: 0.98485 ]\n",
      "train epoch 22: 100%|██████████| 22852/22852 [1:43:17<00:00,  3.69it/s, loss=0.052695 d_precision: 0.98902 d_recall: 0.97925  c_acc: 0.98398 ]\n",
      "test epoch 22: 100%|██████████| 707/707 [01:07<00:00, 10.47it/s, loss=0.051776 d_precision: 0.98032 d_recall: 0.97754  c_acc: 0.98485 ]\n",
      "train epoch 23:  70%|██████▉   | 15968/22852 [1:12:08<31:06,  3.69it/s, loss=0.051717 d_precision: 0.98985 d_recall: 0.97959  c_acc: 0.98387 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4feaa2716239>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"./model_4/detector_{epoch}.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-4feaa2716239>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(detector, corrector, dataloader, epoch, optimizer, scaler)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mloss_corrector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_encodings_original\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_detector\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_corrector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import jieba\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from collections import defaultdict, deque\n",
    "from pypinyin import lazy_pinyin\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 12\n",
    "transformers_path = \"../hfl/chinese-roberta-wwm-ext\"\n",
    "chinese_pinyin_dict_path = \"./resource/chinese_words_frequency.json\"\n",
    "train_dataset_file_path = \"./data/train.json\"\n",
    "test_dataset_file_path = \"./data/test.json\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "jieba.setLogLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class ConfusingSentenceGenerator:\n",
    "    def __init__(self, prob_power_factor=0.7, prob_confuse_phrase=0.1, prob_confuse_word=0.03, prob_similar_sound_word=0.02, prob_random_word=0.02):\n",
    "        self.chinese_pinyin_dict = dict()\n",
    "        self.chinese_single_word_pinyin_list = []\n",
    "        self.chinese_char_regex = re.compile(\"^[\\u4e00-\\u9fff]+$\")\n",
    "        self.prob_power_factor = prob_power_factor\n",
    "        self.prob_confuse_phrase = prob_confuse_phrase\n",
    "        self.prob_confuse_word = prob_confuse_word\n",
    "        self.prob_similar_sound_word = prob_similar_sound_word\n",
    "        self.prob_random_word = prob_random_word\n",
    "        similar_sounds = {\n",
    "            'z_zh': [['za', 'ze', 'zi', 'zu', 'zai', 'zui', 'zao', 'zou', 'zan', 'zen', 'zun', 'zuo', 'zuan', 'zang', 'zeng', 'zong'],\n",
    "                     ['zha', 'zhe', 'zhi', 'zhu', 'zhai', 'zhui', 'zhao', 'zhou', 'zhan', 'zhen', 'zhun', 'zhuo', 'zhuan', 'zhang', 'zheng', 'zhong']],\n",
    "            'c_ch': [['ca', 'ce', 'ci', 'cu', 'cai', 'cui', 'cao', 'cou', 'can', 'cen', 'cun', 'cuo', 'cuan', 'cang', 'ceng', 'cong'],\n",
    "                     ['cha', 'che', 'chi', 'chu', 'chai', 'chui', 'chao', 'chou', 'chan', 'chen', 'chun', 'chuo', 'chuan', 'chang', 'cheng', 'chong']],\n",
    "            's_sh': [['sa', 'se', 'si', 'su', 'sai', 'sui', 'sao', 'sou', 'san', 'sen', 'sun', 'suo', 'suan', 'sang', 'seng'],\n",
    "                     ['sha', 'she', 'shi', 'shu', 'shai', 'shui', 'shao', 'shou', 'shan', 'shen', 'shun', 'shuo', 'shuan', 'shang', 'sheng']],\n",
    "            'l_n': [\n",
    "                ['la', 'le', 'li', 'lu', 'lv', 'lai', 'lei', 'lao', 'lou', 'liu', 'lie', 'liao', 'lian', 'lve', 'lan', 'lin', 'lun', 'luo', 'luan', 'lang', 'leng', 'ling', 'long',\n",
    "                 'liang'],\n",
    "                ['na', 'ne', 'ni', 'nu', 'nv', 'nai', 'nei', 'nao', 'nou', 'niu', 'nie', 'niao', 'nian', 'nve', 'nan', 'nin', 'nun', 'nuo', 'nuan', 'nang', 'neng', 'ning', 'nong',\n",
    "                 'niang']],\n",
    "            'f_h': [['fa', 'fu', 'fei', 'fou', 'fan', 'fen', 'fang', 'feng'],\n",
    "                    ['ha', 'hu', 'hei', 'hou', 'han', 'hen', 'hang', 'heng']],\n",
    "            'r_l': [['re', 'ri', 'ru', 'rao', 'rou', 'ran', 'ren', 'run', 'ruo', 'rang', 'reng', 'rong'],\n",
    "                    ['le', 'li', 'lu', 'lao', 'lou', 'lan', 'len', 'lun', 'luo', 'lang', 'leng', 'long']],\n",
    "            'an_ang': [['ban', 'pan', 'man', 'fan', 'dan', 'tan', 'nan', 'lan', 'gan', 'kan', 'han', 'zhan', 'chan', 'shan', 'ran', 'zan', 'can', 'san', 'yan', 'wan'],\n",
    "                       ['bang', 'pang', 'mang', 'fang', 'dang', 'tang', 'nang', 'lang', 'gang', 'kang', 'hang', 'zhang', 'chang', 'shang', 'rang', 'zang', 'cang', 'sang', 'yang',\n",
    "                        'wang']],\n",
    "            'en_eng': [['ben', 'pen', 'men', 'fen', 'den', 'nen', 'gen', 'ken', 'hen', 'zhen', 'chen', 'shen', 'ren', 'zen', 'cen', 'sen', 'wen'],\n",
    "                       ['beng', 'peng', 'meng', 'feng', 'deng', 'neng', 'geng', 'keng', 'heng', 'zheng', 'cheng', 'sheng', 'reng', 'zeng', 'ceng', 'seng', 'weng']],\n",
    "            'in_ing': [['bin', 'pin', 'min', 'nin', 'lin', 'jin', 'qin', 'xin', 'yin'],\n",
    "                       ['bing', 'ping', 'ming', 'ning', 'ling', 'jing', 'qing', 'xing', 'ying']],\n",
    "            'ian_iang': [['nian', 'lian', 'jian', 'qian', 'xian'],\n",
    "                         ['niang', 'liang', 'jiang', 'qiang', 'xiang']],\n",
    "            'uan_uang': [['guan', 'kuan', 'huan', 'zhuan', 'chuan', 'shuan'],\n",
    "                         ['guang', 'kuang', 'huang', 'zhuang', 'chuang', 'shuang']]\n",
    "        }\n",
    "\n",
    "        similar_sound_dict = defaultdict(set)\n",
    "        for k, v in similar_sounds.items():\n",
    "            for pair in zip(*v):\n",
    "                similar_sound_dict[pair[0]].add(pair[1])\n",
    "                similar_sound_dict[pair[1]].add(pair[0])\n",
    "        self.similar_sound_dict = {k: list(v) for k, v in similar_sound_dict.items()}\n",
    "\n",
    "    def build_dictionary(self, corpus: List[str], chinese_pinyin_dict_path: str):\n",
    "        p_bar = tqdm(corpus, position=0, leave=True)\n",
    "        p_bar.set_description(\"build_chinese_pinyin_dict\")\n",
    "        chinese_words_frequency = defaultdict(int)\n",
    "        for corpus_sentence in p_bar:\n",
    "            for word in jieba.lcut(corpus_sentence):\n",
    "                if self.chinese_char_regex.match(word):\n",
    "                    chinese_words_frequency[word] += 1\n",
    "\n",
    "        chinese_pinyin_dict = defaultdict(list)\n",
    "        for word in chinese_words_frequency:\n",
    "            pinyin_ = \"_\".join(lazy_pinyin(word))\n",
    "            chinese_pinyin_dict[pinyin_].append([word, chinese_words_frequency[word]])\n",
    "\n",
    "        chinese_pinyin_dict = {k: sorted(v, key=lambda x: x[1], reverse=True) for k, v in chinese_pinyin_dict.items()}\n",
    "        chinese_pinyin_dict = {k: [[x[0] for x in v], [x[1] for x in v]] for k, v in chinese_pinyin_dict.items()}\n",
    "\n",
    "        with open(chinese_pinyin_dict_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(chinese_pinyin_dict, f, ensure_ascii=False)\n",
    "\n",
    "    def load_dictionary(self, chinese_pinyin_dict_path: str):\n",
    "        with open(chinese_pinyin_dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.chinese_pinyin_dict = json.load(f)\n",
    "            self.chinese_single_word_pinyin_list = [x for x in self.chinese_pinyin_dict if \"_\" not in x]\n",
    "\n",
    "    def generate_sample(self, original_sentence: str):\n",
    "        confusing_sentence = \"\"\n",
    "        # 一定概率替换相同拼音的词组\n",
    "        for word in jieba.lcut(original_sentence):\n",
    "            if random.random() < self.prob_confuse_phrase:\n",
    "                if self.chinese_char_regex.match(word):\n",
    "                    pinyin_ = \"_\".join(lazy_pinyin(word))\n",
    "                    if pinyin_ in self.chinese_pinyin_dict:\n",
    "                        words_, freq_ = self.chinese_pinyin_dict[pinyin_]\n",
    "                        if word in words_:\n",
    "                            freq_[words_.index(word)] = 1e-4\n",
    "                        freq_ = np.asarray(freq_)\n",
    "                        freq_ = np.power(freq_, self.prob_power_factor)\n",
    "                        freq_ = freq_ / np.sum(freq_)\n",
    "                        confusing_sentence += np.random.choice(words_, p=freq_)\n",
    "                        continue\n",
    "            confusing_sentence += word\n",
    "        confusing_sentence = list(confusing_sentence)\n",
    "        # 一定概率替换相同拼音的字\n",
    "        for i, word in enumerate(confusing_sentence):\n",
    "            if random.random() < self.prob_confuse_word:\n",
    "                if self.chinese_char_regex.match(word):\n",
    "                    pinyin_ = \"_\".join(lazy_pinyin(word))\n",
    "                    if pinyin_ in self.chinese_pinyin_dict:\n",
    "                        words_, freq_ = self.chinese_pinyin_dict[pinyin_]\n",
    "                        if word in words_:\n",
    "                            freq_[words_.index(word)] = 1e-4\n",
    "                        freq_ = np.asarray(freq_)\n",
    "                        freq_ = np.power(freq_, self.prob_power_factor)\n",
    "                        freq_ = freq_ / np.sum(freq_)\n",
    "                        confusing_sentence[i] = np.random.choice(words_, p=freq_)\n",
    "        # 一定概率替换模糊拼音的字\n",
    "        for i, word in enumerate(confusing_sentence):\n",
    "            if random.random() < self.prob_similar_sound_word:\n",
    "                if self.chinese_char_regex.match(word):\n",
    "                    pinyin_ = \"_\".join(lazy_pinyin(word))\n",
    "                    if pinyin_ in self.similar_sound_dict:\n",
    "                        pinyin_ = random.choice(self.similar_sound_dict[pinyin_])\n",
    "                        if pinyin_ in self.chinese_pinyin_dict:\n",
    "                            words_, freq_ = self.chinese_pinyin_dict[pinyin_]\n",
    "                            if word in words_:\n",
    "                                freq_[words_.index(word)] = 1e-4\n",
    "                            freq_ = np.asarray(freq_)\n",
    "                            freq_ = np.power(freq_, self.prob_power_factor)\n",
    "                            freq_ = freq_ / np.sum(freq_)\n",
    "                            confusing_sentence[i] = np.random.choice(words_, p=freq_)\n",
    "        # 一定概率随机替换字\n",
    "        for i, word in enumerate(confusing_sentence):\n",
    "            if random.random() < self.prob_confuse_word:\n",
    "                if self.chinese_char_regex.match(word):\n",
    "                    pinyin_ = random.choice(self.chinese_single_word_pinyin_list)\n",
    "\n",
    "                    words_, freq_ = self.chinese_pinyin_dict[pinyin_]\n",
    "                    if word in words_:\n",
    "                        freq_[words_.index(word)] = 1e-4\n",
    "                    freq_ = np.asarray(freq_)\n",
    "                    freq_ = np.power(freq_, self.prob_power_factor)\n",
    "                    freq_ = freq_ / np.sum(freq_)\n",
    "                    confusing_sentence[i] = np.random.choice(words_, p=freq_)\n",
    "\n",
    "        confusing_sentence = \"\".join(confusing_sentence)\n",
    "        assert len(original_sentence) == len(confusing_sentence)\n",
    "        label = [int(c1 != c2) for c1, c2 in zip(original_sentence, confusing_sentence)]\n",
    "        return {\n",
    "            \"original_sentence\": original_sentence,\n",
    "            \"confusing_sentence\": confusing_sentence,\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "\n",
    "class SampleEncoder:\n",
    "    def __init__(self, transformers_path, max_length=128):\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(transformers_path)\n",
    "\n",
    "    def encode_sentence(self, sentence):\n",
    "        tokens = [self.tokenizer.cls_token] + list(sentence)\n",
    "        tokens = tokens[:self.max_length - 1] + [self.tokenizer.sep_token]\n",
    "        tokens = tokens + [self.tokenizer.pad_token for _ in range(self.max_length - len(tokens))]\n",
    "\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1 if token != self.tokenizer.pad_token else 0 for token in tokens]\n",
    "        token_type_ids = [0 for token in tokens]\n",
    "\n",
    "        _input_encoding = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids\n",
    "        }\n",
    "        _input_encoding = {k: torch.tensor(v, dtype=torch.long) for k, v in _input_encoding.items()}\n",
    "        return _input_encoding\n",
    "\n",
    "    def encode_label(self, label):\n",
    "        label = [0] + label\n",
    "        label = label[:self.max_length - 1] + [0]\n",
    "        label = label + [0 for _ in range(self.max_length - len(label))]\n",
    "        label = np.asarray(label)\n",
    "        return label\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, dataset_file_path):\n",
    "        with open(dataset_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.dataset = json.load(f)\n",
    "        self.dataset_length = len(self.dataset)\n",
    "        print(f\"load train dataset size: {self.dataset_length}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.dataset[idx]\n",
    "        sample = confusing_sentence_generator.generate_sample(sentence)\n",
    "        return (\n",
    "            sample_encoder.encode_sentence(sample[\"original_sentence\"]),\n",
    "            sample_encoder.encode_sentence(sample[\"confusing_sentence\"]),\n",
    "            sample_encoder.encode_label(sample[\"label\"])\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataset_file_path):\n",
    "        with open(dataset_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.dataset = json.load(f)\n",
    "        self.dataset_length = len(self.dataset)\n",
    "        print(f\"load test dataset size: {self.dataset_length}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        return (\n",
    "            sample_encoder.encode_sentence(sample[\"original_sentence\"]),\n",
    "            sample_encoder.encode_sentence(sample[\"confusing_sentence\"]),\n",
    "            sample_encoder.encode_label(sample[\"label\"])\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "\n",
    "confusing_sentence_generator = ConfusingSentenceGenerator()\n",
    "confusing_sentence_generator.load_dictionary(chinese_pinyin_dict_path)\n",
    "sample_encoder = SampleEncoder(transformers_path)\n",
    "\n",
    "\n",
    "class BertDetector(nn.Module):\n",
    "    def __init__(self, transformers_path):\n",
    "        super(BertDetector, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(transformers_path)\n",
    "        self.config = self.bert.config\n",
    "        self.linear_detector = nn.Linear(self.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)[0]\n",
    "        err_prob = self.linear_detector(bert_output)\n",
    "        return err_prob.squeeze(dim=-1)\n",
    "\n",
    "\n",
    "class BertCorrector(nn.Module):\n",
    "    def __init__(self, transformers_path):\n",
    "        super(BertCorrector, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(transformers_path)\n",
    "        self.config = self.bert.config\n",
    "        self.linear_char_predict = nn.Linear(self.config.hidden_size, self.config.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)[0]\n",
    "        char_predict = self.linear_char_predict(bert_output)\n",
    "        return char_predict\n",
    "\n",
    "\n",
    "def train(detector, corrector, dataloader, epoch, optimizer, scaler):\n",
    "    time.sleep(0.2)\n",
    "    detector.train()\n",
    "    corrector.train()\n",
    "    loss_count = deque([], maxlen=100)\n",
    "    detector_tp_count = deque([], maxlen=100)\n",
    "    detector_fp_count = deque([], maxlen=100)\n",
    "    detector_fn_count = deque([], maxlen=100)\n",
    "    detector_tn_count = deque([], maxlen=100)\n",
    "    corrector_accuracy_count = deque([], maxlen=100)\n",
    "    pbar = tqdm(dataloader, position=0, leave=True)\n",
    "    pbar.set_description(\"train epoch {}\".format(epoch))\n",
    "    for input_encodings_original, input_encodings_confusing, y_target in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        input_encodings_original = {k: v.to(device) for k, v in input_encodings_original.items()}\n",
    "        input_encodings_confusing = {k: v.to(device) for k, v in input_encodings_confusing.items()}\n",
    "        y_target = y_target.to(device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            err_prob = detector(**input_encodings_confusing)\n",
    "            char_predict = corrector(**input_encodings_confusing)\n",
    "            loss_detector = F.binary_cross_entropy_with_logits(err_prob, y_target.float())\n",
    "            loss_corrector = F.cross_entropy(char_predict.transpose(-1, -2), input_encodings_original[\"input_ids\"])\n",
    "            loss = loss_detector + loss_corrector\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loss_count.append(loss.item())\n",
    "\n",
    "        y_detector_predict = torch.gt(err_prob, 0)\n",
    "        detector_tp_count.append(torch.logical_and(y_detector_predict, y_target).sum().item())\n",
    "        detector_fp_count.append(torch.logical_and(y_detector_predict, torch.logical_not(y_target)).sum().item())\n",
    "        detector_fn_count.append(torch.logical_and(torch.logical_not(y_detector_predict), y_target).sum().item())\n",
    "        detector_tn_count.append(torch.logical_and(torch.logical_not(y_detector_predict), torch.logical_not(y_target)).sum().item())\n",
    "\n",
    "        y_corrector_predict = torch.eq(torch.argmax(char_predict, dim=-1), input_encodings_original[\"input_ids\"])\n",
    "        corrector_accuracy_count.append(y_corrector_predict.sum().item() / torch.ones_like(y_corrector_predict).sum().item())\n",
    "\n",
    "        cur_loss = np.mean(loss_count)\n",
    "        cur_precision = np.sum(detector_tp_count) / (np.sum(detector_tp_count) + np.sum(detector_fp_count) + 1e-5)\n",
    "        cur_recall = np.sum(detector_tp_count) / (np.sum(detector_tp_count) + np.sum(detector_fn_count) + 1e-5)\n",
    "        cur_acc = np.mean(corrector_accuracy_count)\n",
    "\n",
    "        log_str = f\"loss={cur_loss:>6.5} d_precision:{cur_precision:>8.5} d_recall:{cur_recall:>8.5}  c_acc:{cur_acc:>8.5} \"\n",
    "        pbar.set_postfix_str(log_str)\n",
    "\n",
    "\n",
    "def test(detector, corrector, dataloader, epoch):\n",
    "    time.sleep(0.2)\n",
    "    detector.eval()\n",
    "    corrector.eval()\n",
    "    loss_count = []\n",
    "    detector_tp_count = []\n",
    "    detector_fp_count = []\n",
    "    detector_fn_count = []\n",
    "    detector_tn_count = []\n",
    "    corrector_accuracy_count = []\n",
    "    pbar = tqdm(dataloader, position=0, leave=True)\n",
    "    pbar.set_description(\"test epoch {}\".format(epoch))\n",
    "    for input_encodings_original, input_encodings_confusing, y_target in pbar:\n",
    "        input_encodings_original = {k: v.to(device) for k, v in input_encodings_original.items()}\n",
    "        input_encodings_confusing = {k: v.to(device) for k, v in input_encodings_confusing.items()}\n",
    "        y_target = y_target.to(device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            err_prob = detector(**input_encodings_confusing)\n",
    "            char_predict = corrector(**input_encodings_confusing)\n",
    "            loss_detector = F.binary_cross_entropy_with_logits(err_prob, y_target.float())\n",
    "            loss_corrector = F.cross_entropy(char_predict.transpose(-1, -2), input_encodings_original[\"input_ids\"])\n",
    "            loss = loss_detector + loss_corrector\n",
    "\n",
    "        loss_count.append(loss.item())\n",
    "\n",
    "        y_detector_predict = torch.gt(err_prob, 0)\n",
    "        detector_tp_count.append(torch.logical_and(y_detector_predict, y_target).sum().item())\n",
    "        detector_fp_count.append(torch.logical_and(y_detector_predict, torch.logical_not(y_target)).sum().item())\n",
    "        detector_fn_count.append(torch.logical_and(torch.logical_not(y_detector_predict), y_target).sum().item())\n",
    "        detector_tn_count.append(torch.logical_and(torch.logical_not(y_detector_predict), torch.logical_not(y_target)).sum().item())\n",
    "\n",
    "        y_corrector_predict = torch.eq(torch.argmax(char_predict, dim=-1), input_encodings_original[\"input_ids\"])\n",
    "        corrector_accuracy_count.append(y_corrector_predict.sum().item() / torch.ones_like(y_corrector_predict).sum().item())\n",
    "\n",
    "        cur_loss = np.mean(loss_count)\n",
    "        cur_precision = np.sum(detector_tp_count) / (np.sum(detector_tp_count) + np.sum(detector_fp_count) + 1e-5)\n",
    "        cur_recall = np.sum(detector_tp_count) / (np.sum(detector_tp_count) + np.sum(detector_fn_count) + 1e-5)\n",
    "        cur_acc = np.mean(corrector_accuracy_count)\n",
    "\n",
    "        log_str = f\"loss={cur_loss:>6.5} d_precision:{cur_precision:>8.5} d_recall:{cur_recall:>8.5}  c_acc:{cur_acc:>8.5} \"\n",
    "        pbar.set_postfix_str(log_str)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset_train = TrainDataset(train_dataset_file_path)\n",
    "    dataset_test = TestDataset(test_dataset_file_path)\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    detector = BertDetector(transformers_path)\n",
    "    corrector = BertCorrector(transformers_path)\n",
    "    detector.to(device)\n",
    "    corrector.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(detector.parameters()) + list(corrector.parameters()), lr=1e-5)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(100):\n",
    "        train(detector, corrector, dataloader_train, epoch, optimizer, scaler)\n",
    "        test(detector, corrector, dataloader_test, epoch)\n",
    "        torch.save(detector.state_dict(), f\"./model_4/detector_{epoch}.pth\")\n",
    "        torch.save(corrector.state_dict(), f\"./model_4/corrector_{epoch}.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
