{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5710ce0-8a68-4654-877e-255ba875e5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 25 07:04:01 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.27       Driver Version: 465.27       CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA Tesla V1...  Off  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   48C    P0    41W / 250W |   6019MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d63760b-ff46-4c9d-a334-77231907f396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /opt/conda/lib/python3.7/site-packages (0.42.1)\n",
      "Requirement already satisfied: pypinyin in /opt/conda/lib/python3.7/site-packages (0.42.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.9.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.5.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U jieba pypinyin transformers --proxy http://10.8.84.123:7890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e351bd-823e-4524-a7c0-0c15c31f87bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load train dataset size: 731254\n",
      "load test dataset size: 22617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 0: 100%|██████████| 22852/22852 [1:04:47<00:00,  5.88it/s, loss=0.09164 d_precision: 0.94493 d_recall: 0.93055  c_acc: 0.96957 ] \n",
      "test epoch 0: 100%|██████████| 707/707 [00:45<00:00, 15.58it/s, loss=0.084392 d_precision: 0.95958 d_recall: 0.92572  c_acc: 0.97108 ]\n",
      "train epoch 1: 100%|██████████| 22852/22852 [1:04:55<00:00,  5.87it/s, loss=0.062023 d_precision: 0.95906 d_recall: 0.94778  c_acc: 0.97616 ]\n",
      "test epoch 1: 100%|██████████| 707/707 [00:44<00:00, 15.72it/s, loss=0.058077 d_precision: 0.96756 d_recall: 0.94324  c_acc: 0.97722 ]\n",
      "train epoch 2: 100%|██████████| 22852/22852 [1:04:47<00:00,  5.88it/s, loss=0.052886 d_precision: 0.96546 d_recall:   0.952  c_acc: 0.97837 ]\n",
      "test epoch 2: 100%|██████████| 707/707 [00:45<00:00, 15.64it/s, loss=0.049418 d_precision: 0.96817 d_recall: 0.95166  c_acc: 0.97956 ]\n",
      "train epoch 3: 100%|██████████| 22852/22852 [1:05:10<00:00,  5.84it/s, loss=0.046217 d_precision:  0.9689 d_recall: 0.95628  c_acc: 0.97971 ]\n",
      "test epoch 3: 100%|██████████| 707/707 [00:45<00:00, 15.64it/s, loss=0.044515 d_precision: 0.97376 d_recall: 0.95289  c_acc: 0.98085 ]\n",
      "train epoch 4: 100%|██████████| 22852/22852 [1:04:32<00:00,  5.90it/s, loss=0.044394 d_precision: 0.97012 d_recall:  0.9595  c_acc: 0.98046 ]\n",
      "test epoch 4: 100%|██████████| 707/707 [00:45<00:00, 15.56it/s, loss=0.041613 d_precision: 0.97648 d_recall: 0.95457  c_acc: 0.98162 ]\n",
      "train epoch 5: 100%|██████████| 22852/22852 [1:04:59<00:00,  5.86it/s, loss=0.041584 d_precision: 0.97223 d_recall: 0.96068  c_acc: 0.98117 ]\n",
      "test epoch 5: 100%|██████████| 707/707 [00:45<00:00, 15.57it/s, loss=0.039222 d_precision: 0.97346 d_recall: 0.96182  c_acc: 0.98222 ]\n",
      "train epoch 6: 100%|██████████| 22852/22852 [1:04:47<00:00,  5.88it/s, loss=0.038448 d_precision: 0.97585 d_recall: 0.96307  c_acc:  0.9821 ]\n",
      "test epoch 6: 100%|██████████| 707/707 [00:45<00:00, 15.63it/s, loss=0.037646 d_precision: 0.97551 d_recall: 0.96242  c_acc: 0.98262 ]\n",
      "train epoch 7: 100%|██████████| 22852/22852 [1:04:51<00:00,  5.87it/s, loss=0.038727 d_precision: 0.97589 d_recall: 0.96413  c_acc: 0.98213 ]\n",
      "test epoch 7: 100%|██████████| 707/707 [00:44<00:00, 15.73it/s, loss=0.036431 d_precision: 0.97758 d_recall: 0.96267  c_acc: 0.98299 ]\n",
      "train epoch 8: 100%|██████████| 22852/22852 [1:04:50<00:00,  5.87it/s, loss=0.036787 d_precision: 0.97888 d_recall: 0.96605  c_acc: 0.98247 ]\n",
      "test epoch 8: 100%|██████████| 707/707 [00:45<00:00, 15.56it/s, loss=0.035526 d_precision: 0.97544 d_recall: 0.96612  c_acc:  0.9832 ]\n",
      "train epoch 9: 100%|██████████| 22852/22852 [1:04:58<00:00,  5.86it/s, loss=0.034871 d_precision: 0.97997 d_recall: 0.96796  c_acc: 0.98304 ]\n",
      "test epoch 9: 100%|██████████| 707/707 [00:45<00:00, 15.54it/s, loss=0.034604 d_precision: 0.97801 d_recall: 0.96566  c_acc:  0.9835 ]\n",
      "train epoch 10: 100%|██████████| 22852/22852 [1:04:57<00:00,  5.86it/s, loss=0.03526 d_precision: 0.97831 d_recall: 0.96781  c_acc: 0.98284 ] \n",
      "test epoch 10: 100%|██████████| 707/707 [00:45<00:00, 15.54it/s, loss=0.033766 d_precision: 0.97635 d_recall: 0.96879  c_acc: 0.98372 ]\n",
      "train epoch 11: 100%|██████████| 22852/22852 [1:05:00<00:00,  5.86it/s, loss=0.035092 d_precision: 0.97853 d_recall: 0.96934  c_acc: 0.98299 ]\n",
      "test epoch 11: 100%|██████████| 707/707 [00:45<00:00, 15.54it/s, loss=0.033207 d_precision: 0.97711 d_recall: 0.96984  c_acc: 0.98385 ]\n",
      "train epoch 12: 100%|██████████| 22852/22852 [1:04:58<00:00,  5.86it/s, loss=0.034007 d_precision: 0.98153 d_recall: 0.97084  c_acc: 0.98336 ]\n",
      "test epoch 12: 100%|██████████| 707/707 [00:45<00:00, 15.67it/s, loss=0.032705 d_precision: 0.97871 d_recall: 0.96851  c_acc:   0.984 ]\n",
      "train epoch 13: 100%|██████████| 22852/22852 [1:05:00<00:00,  5.86it/s, loss=0.033258 d_precision:  0.9814 d_recall: 0.96984  c_acc: 0.98338 ]\n",
      "test epoch 13: 100%|██████████| 707/707 [00:45<00:00, 15.59it/s, loss=0.032258 d_precision: 0.97791 d_recall: 0.97054  c_acc: 0.98413 ]\n",
      "train epoch 14: 100%|██████████| 22852/22852 [1:04:54<00:00,  5.87it/s, loss=0.032243 d_precision: 0.98265 d_recall: 0.97054  c_acc: 0.98361 ]\n",
      "test epoch 14: 100%|██████████| 707/707 [00:45<00:00, 15.57it/s, loss=0.031824 d_precision: 0.98109 d_recall: 0.96881  c_acc: 0.98427 ]\n",
      "train epoch 15: 100%|██████████| 22852/22852 [1:05:00<00:00,  5.86it/s, loss=0.031626 d_precision: 0.98304 d_recall: 0.97178  c_acc: 0.98389 ]\n",
      "test epoch 15: 100%|██████████| 707/707 [00:45<00:00, 15.56it/s, loss=0.031488 d_precision: 0.97989 d_recall: 0.97067  c_acc: 0.98434 ]\n",
      "train epoch 16:  11%|█         | 2402/22852 [06:50<58:19,  5.84it/s, loss=0.030095 d_precision:  0.9823 d_recall:  0.9718  c_acc: 0.98417 ]  "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "import jieba\n",
    "import logging\n",
    "from pypinyin import lazy_pinyin\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "loss_detector_weight = 0.4\n",
    "num_workers = 12\n",
    "transformers_path = \"../hfl/chinese-roberta-wwm-ext\"\n",
    "chinese_pinyin_dict_path = \"./resource/chinese_words_frequency.json\"\n",
    "train_dataset_file_path = \"./data/train.json\"\n",
    "test_dataset_file_path = \"./data/test.json\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "jieba.setLogLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class ConfusingSentenceGenerator:\n",
    "    def __init__(self, prob_power_factor=0.7, prob_confuse_phrase=0.1, prob_confuse_word=0.03, prob_random_word=0.02):\n",
    "        self.chinese_pinyin_dict = dict()\n",
    "        self.chinese_single_word_pinyin_list = []\n",
    "        self.chinese_char_regex = re.compile(\"^[\\u4e00-\\u9fff]+$\")\n",
    "        self.prob_power_factor = prob_power_factor\n",
    "        self.prob_confuse_phrase = prob_confuse_phrase\n",
    "        self.prob_confuse_word = prob_confuse_word\n",
    "        self.prob_random_word = prob_random_word\n",
    "\n",
    "    def build_dictionary(self, corpus: List[str], chinese_pinyin_dict_path: str):\n",
    "        p_bar = tqdm(corpus, position=0, leave=True)\n",
    "        p_bar.set_description(\"build_chinese_pinyin_dict\")\n",
    "        chinese_words_frequency = defaultdict(int)\n",
    "        for corpus_sentence in p_bar:\n",
    "            for word in jieba.lcut(corpus_sentence):\n",
    "                if self.chinese_char_regex.match(word):\n",
    "                    chinese_words_frequency[word] += 1\n",
    "\n",
    "        chinese_pinyin_dict = defaultdict(list)\n",
    "        for word in chinese_words_frequency:\n",
    "            pinyin_ = \"_\".join(lazy_pinyin(word))\n",
    "            chinese_pinyin_dict[pinyin_].append([word, chinese_words_frequency[word]])\n",
    "\n",
    "        chinese_pinyin_dict = {k: sorted(v, key=lambda x: x[1], reverse=True) for k, v in chinese_pinyin_dict.items()}\n",
    "        chinese_pinyin_dict = {k: [[x[0] for x in v], [x[1] for x in v]] for k, v in chinese_pinyin_dict.items()}\n",
    "\n",
    "        with open(chinese_pinyin_dict_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(chinese_pinyin_dict, f, ensure_ascii=False)\n",
    "\n",
    "    def load_dictionary(self, chinese_pinyin_dict_path: str):\n",
    "        with open(chinese_pinyin_dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.chinese_pinyin_dict = json.load(f)\n",
    "            self.chinese_single_word_pinyin_list = [x for x in self.chinese_pinyin_dict if \"_\" not in x]\n",
    "\n",
    "    def generate_sample(self, original_sentence: str):\n",
    "        confusing_sentence = \"\"\n",
    "        # 一定概率替换相同拼音的词组\n",
    "        for word in jieba.lcut(original_sentence):\n",
    "            if random.random() < self.prob_confuse_phrase:\n",
    "                if self.chinese_char_regex.match(word):\n",
    "                    pinyin_ = \"_\".join(lazy_pinyin(word))\n",
    "                    if pinyin_ in self.chinese_pinyin_dict:\n",
    "                        words_, freq_ = self.chinese_pinyin_dict[pinyin_]\n",
    "                        if word in words_:\n",
    "                            freq_[words_.index(word)] = 1e-4\n",
    "                        freq_ = np.asarray(freq_)\n",
    "                        freq_ = np.power(freq_, self.prob_power_factor)\n",
    "                        freq_ = freq_ / np.sum(freq_)\n",
    "                        confusing_sentence += np.random.choice(words_, p=freq_)\n",
    "                        continue\n",
    "            confusing_sentence += word\n",
    "        confusing_sentence = list(confusing_sentence)\n",
    "        # 一定概率替换相同拼音的字\n",
    "        for i, word in enumerate(confusing_sentence):\n",
    "            if random.random() < self.prob_confuse_word:\n",
    "                if self.chinese_char_regex.match(word):\n",
    "                    pinyin_ = \"_\".join(lazy_pinyin(word))\n",
    "                    if pinyin_ in self.chinese_pinyin_dict:\n",
    "                        words_, freq_ = self.chinese_pinyin_dict[pinyin_]\n",
    "                        if word in words_:\n",
    "                            freq_[words_.index(word)] = 1e-4\n",
    "                        freq_ = np.asarray(freq_)\n",
    "                        freq_ = np.power(freq_, self.prob_power_factor)\n",
    "                        freq_ = freq_ / np.sum(freq_)\n",
    "                        confusing_sentence[i] = np.random.choice(words_, p=freq_)\n",
    "        # 一定概率随机替换字\n",
    "        for i, word in enumerate(confusing_sentence):\n",
    "            if random.random() < self.prob_confuse_word:\n",
    "                if self.chinese_char_regex.match(word):\n",
    "                    pinyin_ = random.choice(self.chinese_single_word_pinyin_list)\n",
    "\n",
    "                    words_, freq_ = self.chinese_pinyin_dict[pinyin_]\n",
    "                    if word in words_:\n",
    "                        freq_[words_.index(word)] = 1e-4\n",
    "                    freq_ = np.asarray(freq_)\n",
    "                    freq_ = np.power(freq_, self.prob_power_factor)\n",
    "                    freq_ = freq_ / np.sum(freq_)\n",
    "                    confusing_sentence[i] = np.random.choice(words_, p=freq_)\n",
    "\n",
    "        confusing_sentence = \"\".join(confusing_sentence)\n",
    "        assert len(original_sentence) == len(confusing_sentence)\n",
    "        label = [int(c1 != c2) for c1, c2 in zip(original_sentence, confusing_sentence)]\n",
    "        return {\n",
    "            \"original_sentence\": original_sentence,\n",
    "            \"confusing_sentence\": confusing_sentence,\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "\n",
    "class SampleEncoder:\n",
    "    def __init__(self, transformers_path, max_length=128):\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(transformers_path)\n",
    "\n",
    "    def encode_sentence(self, sentence):\n",
    "        tokens = [self.tokenizer.cls_token] + list(sentence)\n",
    "        tokens = tokens[:self.max_length - 1] + [self.tokenizer.sep_token]\n",
    "        tokens = tokens + [self.tokenizer.pad_token for _ in range(self.max_length - len(tokens))]\n",
    "\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1 if token != self.tokenizer.pad_token else 0 for token in tokens]\n",
    "        token_type_ids = [0 for token in tokens]\n",
    "\n",
    "        _input_encoding = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids\n",
    "        }\n",
    "        _input_encoding = {k: torch.tensor(v, dtype=torch.long) for k, v in _input_encoding.items()}\n",
    "        return _input_encoding\n",
    "\n",
    "    def encode_label(self, label):\n",
    "        label = [0] + label\n",
    "        label = label[:self.max_length - 1] + [0]\n",
    "        label = label + [0 for _ in range(self.max_length - len(label))]\n",
    "        label = np.asarray(label)\n",
    "        return label\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, dataset_file_path):\n",
    "        with open(dataset_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.dataset = json.load(f)\n",
    "        self.dataset_length = len(self.dataset)\n",
    "        print(f\"load train dataset size: {self.dataset_length}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.dataset[idx]\n",
    "        sample = confusing_sentence_generator.generate_sample(sentence)\n",
    "        return (\n",
    "            sample_encoder.encode_sentence(sample[\"original_sentence\"]),\n",
    "            sample_encoder.encode_sentence(sample[\"confusing_sentence\"]),\n",
    "            sample_encoder.encode_label(sample[\"label\"])\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataset_file_path):\n",
    "        with open(dataset_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.dataset = json.load(f)\n",
    "        self.dataset_length = len(self.dataset)\n",
    "        print(f\"load test dataset size: {self.dataset_length}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        return (\n",
    "            sample_encoder.encode_sentence(sample[\"original_sentence\"]),\n",
    "            sample_encoder.encode_sentence(sample[\"confusing_sentence\"]),\n",
    "            sample_encoder.encode_label(sample[\"label\"])\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "\n",
    "confusing_sentence_generator = ConfusingSentenceGenerator()\n",
    "confusing_sentence_generator.load_dictionary(chinese_pinyin_dict_path)\n",
    "sample_encoder = SampleEncoder(transformers_path)\n",
    "\n",
    "\n",
    "class BertCorrector(nn.Module):\n",
    "    def __init__(self, transformers_path):\n",
    "        super(BertCorrector, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(transformers_path)\n",
    "        self.config = self.bert.config\n",
    "        self.linear_detector = nn.Linear(self.config.hidden_size, 1)\n",
    "        self.linear_char_predict = nn.Linear(self.config.hidden_size, self.config.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)[0]\n",
    "        err_prob = self.linear_detector(bert_output)\n",
    "        char_predict = self.linear_char_predict(bert_output)\n",
    "        return err_prob.squeeze(dim=-1), char_predict\n",
    "\n",
    "\n",
    "def train(model, dataloader, epoch, optimizer, scaler):\n",
    "    time.sleep(0.2)\n",
    "    model.train()\n",
    "    loss_count = deque([], maxlen=100)\n",
    "    detector_tp_count = deque([], maxlen=100)\n",
    "    detector_fp_count = deque([], maxlen=100)\n",
    "    detector_fn_count = deque([], maxlen=100)\n",
    "    detector_tn_count = deque([], maxlen=100)\n",
    "    corrector_accuracy_count = deque([], maxlen=100)\n",
    "    pbar = tqdm(dataloader, position=0, leave=True)\n",
    "    pbar.set_description(\"train epoch {}\".format(epoch))\n",
    "    for input_encodings_original, input_encodings_confusing, y_target in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        input_encodings_original = {k: v.to(device) for k, v in input_encodings_original.items()}\n",
    "        input_encodings_confusing = {k: v.to(device) for k, v in input_encodings_confusing.items()}\n",
    "        y_target = y_target.to(device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            err_prob, char_predict = model(**input_encodings_confusing)\n",
    "            loss_detector = F.binary_cross_entropy_with_logits(err_prob, y_target.float())\n",
    "            loss_corrector = F.cross_entropy(char_predict.transpose(-1, -2), input_encodings_original[\"input_ids\"])\n",
    "            loss = loss_detector_weight * loss_detector + (1 - loss_detector_weight) * loss_corrector\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loss_count.append(loss.item())\n",
    "\n",
    "        y_detector_predict = torch.gt(err_prob, 0)\n",
    "        detector_tp_count.append(torch.logical_and(y_detector_predict, y_target).sum().item())\n",
    "        detector_fp_count.append(torch.logical_and(y_detector_predict, torch.logical_not(y_target)).sum().item())\n",
    "        detector_fn_count.append(torch.logical_and(torch.logical_not(y_detector_predict), y_target).sum().item())\n",
    "        detector_tn_count.append(torch.logical_and(torch.logical_not(y_detector_predict), torch.logical_not(y_target)).sum().item())\n",
    "\n",
    "        y_corrector_predict = torch.eq(torch.argmax(char_predict, dim=-1), input_encodings_original[\"input_ids\"])\n",
    "        corrector_accuracy_count.append(y_corrector_predict.sum().item() / torch.ones_like(y_corrector_predict).sum().item())\n",
    "\n",
    "        cur_loss = np.mean(loss_count)\n",
    "        cur_precision = np.sum(detector_tp_count) / (np.sum(detector_tp_count) + np.sum(detector_fp_count) + 1e-5)\n",
    "        cur_recall = np.sum(detector_tp_count) / (np.sum(detector_tp_count) + np.sum(detector_fn_count) + 1e-5)\n",
    "        cur_acc = np.mean(corrector_accuracy_count)\n",
    "\n",
    "        log_str = f\"loss={cur_loss:>6.5} d_precision:{cur_precision:>8.5} d_recall:{cur_recall:>8.5}  c_acc:{cur_acc:>8.5} \"\n",
    "        pbar.set_postfix_str(log_str)\n",
    "\n",
    "\n",
    "def test(model, dataloader, epoch):\n",
    "    time.sleep(0.2)\n",
    "    model.eval()\n",
    "    loss_count = []\n",
    "    detector_tp_count = []\n",
    "    detector_fp_count = []\n",
    "    detector_fn_count = []\n",
    "    detector_tn_count = []\n",
    "    corrector_accuracy_count = []\n",
    "    pbar = tqdm(dataloader, position=0, leave=True)\n",
    "    pbar.set_description(\"test epoch {}\".format(epoch))\n",
    "    for input_encodings_original, input_encodings_confusing, y_target in pbar:\n",
    "        input_encodings_original = {k: v.to(device) for k, v in input_encodings_original.items()}\n",
    "        input_encodings_confusing = {k: v.to(device) for k, v in input_encodings_confusing.items()}\n",
    "        y_target = y_target.to(device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            err_prob, char_predict = model(**input_encodings_confusing)\n",
    "            loss_detector = F.binary_cross_entropy_with_logits(err_prob, y_target.float())\n",
    "            loss_corrector = F.cross_entropy(char_predict.transpose(-1, -2), input_encodings_original[\"input_ids\"])\n",
    "            loss = loss_detector_weight * loss_detector + (1 - loss_detector_weight) * loss_corrector\n",
    "\n",
    "        loss_count.append(loss.item())\n",
    "\n",
    "        y_detector_predict = torch.gt(err_prob, 0)\n",
    "        detector_tp_count.append(torch.logical_and(y_detector_predict, y_target).sum().item())\n",
    "        detector_fp_count.append(torch.logical_and(y_detector_predict, torch.logical_not(y_target)).sum().item())\n",
    "        detector_fn_count.append(torch.logical_and(torch.logical_not(y_detector_predict), y_target).sum().item())\n",
    "        detector_tn_count.append(torch.logical_and(torch.logical_not(y_detector_predict), torch.logical_not(y_target)).sum().item())\n",
    "\n",
    "        y_corrector_predict = torch.eq(torch.argmax(char_predict, dim=-1), input_encodings_original[\"input_ids\"])\n",
    "        corrector_accuracy_count.append(y_corrector_predict.sum().item() / torch.ones_like(y_corrector_predict).sum().item())\n",
    "\n",
    "        cur_loss = np.mean(loss_count)\n",
    "        cur_precision = np.sum(detector_tp_count) / (np.sum(detector_tp_count) + np.sum(detector_fp_count) + 1e-5)\n",
    "        cur_recall = np.sum(detector_tp_count) / (np.sum(detector_tp_count) + np.sum(detector_fn_count) + 1e-5)\n",
    "        cur_acc = np.mean(corrector_accuracy_count)\n",
    "\n",
    "        log_str = f\"loss={cur_loss:>6.5} d_precision:{cur_precision:>8.5} d_recall:{cur_recall:>8.5}  c_acc:{cur_acc:>8.5} \"\n",
    "        pbar.set_postfix_str(log_str)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset_train = TrainDataset(train_dataset_file_path)\n",
    "    dataset_test = TestDataset(test_dataset_file_path)\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    model = BertCorrector(transformers_path)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(100):\n",
    "        train(model, dataloader_train, epoch, optimizer, scaler)\n",
    "        test(model, dataloader_test, epoch)\n",
    "        torch.save(model.state_dict(), f\"./model_1/model_{epoch}.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
