{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20210616_text_multi_label_cls_focal_loss.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO0VOxFN1X5kw1WR28vBAAu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"NNZypW6w-EuS"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import json\n","from tqdm import tqdm\n","import numpy as np\n","from collections import deque, defaultdict\n","import time\n","\n","# train_dataset_file_path = \"D:/db/news/THUCNews/train_3.json\"\n","# test_dataset_file_path = \"D:/db/news/THUCNews/test_3.json\"\n","train_dataset_file_path = \"./train_3.json\"\n","test_dataset_file_path = \"./test_3.json\"\n","transformer_pretrained_path = \"../hfl/chinese-roberta-wwm-ext\"\n","\n","batch_size = 256\n","yamma = 3\n","\n","labels = ['体育', '娱乐', '家居', '彩票', '房产', '教育', '时尚', '时政', '星座', '游戏', '社会', '科技', '股票', '财经']\n","cls_id_dict = {x: i for (i, x) in enumerate(labels)}\n","\n","labels_parsed = [\"时政\", \"财经\", \"房产\", \"科技\"]\n","\n","\n","class TextPreprocessor:\n","    def __init__(self):\n","        self.tokenizer = BertTokenizer.from_pretrained(transformer_pretrained_path)\n","\n","    def preprocess(self, input_title: str):\n","        input_encoding = self.tokenizer(input_title,\n","                                        padding=\"max_length\", truncation=True,\n","                                        max_length=32, return_tensors=\"pt\")\n","        input_encoding = {k: v.squeeze() for k, v in input_encoding.items()}\n","        return input_encoding\n","\n","\n","class NewsDataset(Dataset):\n","    def __init__(self, train: bool):\n","        self.train = train\n","        self.text_preprocessor = TextPreprocessor()\n","        self.dataset_file_path = train_dataset_file_path if self.train else test_dataset_file_path\n","        with open(self.dataset_file_path, encoding=\"utf-8\") as file_:\n","            self.dataset = json.load(file_)\n","        np.random.shuffle(self.dataset)\n","        self.dataset_length = len(self.dataset)\n","        print(f\"load {'train' if self.train else 'test'} dataset size: {self.dataset_length}\")\n","\n","    def __getitem__(self, index):\n","        sample = self.dataset[index]\n","        sample_title = sample[\"title\"]\n","        sample_field_name = sample[\"label\"]\n","        label_parsed = \"None\"\n","        if sample_field_name == \"时政\":\n","            label_parsed = \"时政\"\n","        elif sample_field_name in [\"股票\", \"财经\"]:\n","            label_parsed = \"财经\"\n","        elif sample_field_name == \"房产\":\n","            label_parsed = \"房产\"\n","        elif sample_field_name == \"科技\":\n","            label_parsed = \"科技\"\n","\n","        y = np.array([x == label_parsed for x in labels_parsed], dtype=int)\n","        input_encoding = self.text_preprocessor.preprocess(sample_title)\n","        return input_encoding, y\n","\n","    def __len__(self):\n","        return self.dataset_length\n","\n","\n","def train():\n","    time.sleep(0.2)\n","    model.train()\n","    train_loss = deque([], maxlen=100)\n","    TP_count = defaultdict(int)\n","    FP_count = defaultdict(int)\n","    FN_count = defaultdict(int)\n","    TN_count = defaultdict(int)\n","    pbar = tqdm(dataloader_train)\n","    pbar.set_description(\"train epoch {}\".format(epoch))\n","    for input_encoding, y_target in pbar:\n","        optimizer.zero_grad()\n","        input_encoding = {k: v.to(device) for k, v in input_encoding.items()}\n","        y_target = y_target.to(device)\n","        with torch.cuda.amp.autocast():\n","            y_predict = model(**input_encoding)[0]\n","\n","            bce_loss = F.binary_cross_entropy_with_logits(y_predict, y_target.float(), reduction='none')\n","            focal_loss = torch.pow(1 - torch.exp(-bce_loss), yamma) * bce_loss\n","            loss = torch.mean(focal_loss)\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        train_loss.append(loss.item())\n","\n","        y_predict = torch.sigmoid(y_predict)\n","        for i, label_str in enumerate(labels_parsed):\n","            y_predict_label = torch.gt(y_predict[..., i], 0.5)\n","            y_target_label = torch.eq(y_target[..., i], 1)\n","            TP_count[label_str] += torch.logical_and(y_predict_label, y_target_label).sum().item()\n","            FP_count[label_str] += torch.logical_and(y_predict_label, torch.logical_not(y_target_label)).sum().item()\n","            FN_count[label_str] += torch.logical_and(torch.logical_not(y_predict_label), y_target_label).sum().item()\n","            TN_count[label_str] += torch.logical_and(torch.logical_not(y_predict_label), torch.logical_not(y_target_label)).sum().item()\n","\n","        log_str = \"loss={}\".format(np.mean(train_loss))\n","        pbar.set_postfix_str(log_str)\n","    for i, label_str in enumerate(labels_parsed):\n","        nums = TP_count[label_str] + FN_count[label_str]\n","        precision = TP_count[label_str] / (TP_count[label_str] + FP_count[label_str] + 1e-5)\n","        recall = TP_count[label_str] / (TP_count[label_str] + FN_count[label_str] + 1e-5)\n","        f1 = (2 * precision * recall) / (precision + recall + 1e-5)\n","        print(f\"label {label_str}: precision={precision}, recall={recall}, f1={f1}, nums={nums}\")\n","\n","\n","def test():\n","    time.sleep(0.2)\n","    model.eval()\n","    test_loss = []\n","    TP_count = defaultdict(int)\n","    FP_count = defaultdict(int)\n","    FN_count = defaultdict(int)\n","    TN_count = defaultdict(int)\n","    pbar = tqdm(dataloader_test)\n","    pbar.set_description(\"test epoch {}\".format(epoch))\n","    for input_encoding, y_target in pbar:\n","        input_encoding = {k: v.to(device) for k, v in input_encoding.items()}\n","        y_target = y_target.to(device)\n","        y_predict = model(**input_encoding)[0]\n","\n","        bce_loss = F.binary_cross_entropy_with_logits(y_predict, y_target.float(), reduction='none')\n","        focal_loss = torch.pow(1 - torch.exp(-bce_loss), yamma) * bce_loss\n","        loss = torch.mean(focal_loss)\n","        test_loss.append(loss.item())\n","\n","        y_predict = torch.sigmoid(y_predict)\n","        for i, label_str in enumerate(labels_parsed):\n","            y_predict_label = torch.gt(y_predict[..., i], 0.5)\n","            y_target_label = torch.eq(y_target[..., i], 1)\n","\n","            TP_count[label_str] += torch.logical_and(y_predict_label, y_target_label).sum().item()\n","            FP_count[label_str] += torch.logical_and(y_predict_label, torch.logical_not(y_target_label)).sum().item()\n","            FN_count[label_str] += torch.logical_and(torch.logical_not(y_predict_label), y_target_label).sum().item()\n","            TN_count[label_str] += torch.logical_and(torch.logical_not(y_predict_label), torch.logical_not(y_target_label)).sum().item()\n","\n","        log_str = f\"loss={np.mean(test_loss)}\"\n","        pbar.set_postfix_str(log_str)\n","\n","    for i, label_str in enumerate(labels_parsed):\n","        nums = TP_count[label_str] + FN_count[label_str]\n","        precision = TP_count[label_str] / (TP_count[label_str] + FP_count[label_str] + 1e-5)\n","        recall = TP_count[label_str] / (TP_count[label_str] + FN_count[label_str] + 1e-5)\n","        f1 = (2 * precision * recall) / (precision + recall + 1e-5)\n","        print(f\"label {label_str}: precision={precision}, recall={recall}, f1={f1}, nums={nums}\")\n","\n","\n","if __name__ == '__main__':\n","    dataset_train = NewsDataset(train=True)\n","    dataloader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True, num_workers=12, pin_memory=True)\n","    dataset_test = NewsDataset(train=False)\n","    dataloader_test = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False, num_workers=12, pin_memory=True)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = BertForSequenceClassification.from_pretrained(transformer_pretrained_path, num_labels=4)\n","    model.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n","\n","    scaler = torch.cuda.amp.GradScaler()\n","\n","    for epoch in range(100):\n","        train()\n","        test()\n","        torch.save(model.state_dict(), f\"./model_6/model_{epoch}.pth\")\n"],"execution_count":null,"outputs":[]}]}