{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20210718_huggingface_to_onnx_util.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMt/0gYUg9jd2bWgC2B05FX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fgTyrgr_sL-3","executionInfo":{"status":"ok","timestamp":1626579529193,"user_tz":-480,"elapsed":7,"user":{"displayName":"Junyi Zou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiI4SN2nHAy9s4orcyo-toorAEOB858CzrJ0JiH=s64","userId":"15286086218084006415"}},"outputId":"bf477259-3038-4736-982e-0a47cc71e460"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Sun Jul 18 03:39:51 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I8vTxH3RsgbU"},"source":["import torch\n","from transformers import BertForSequenceClassification\n","from transformers.convert_graph_to_onnx import convert\n","from onnxruntime.transformers import optimizer\n","from pathlib import Path\n","import os\n","\n","\n","def convert_bert_classification_model_to_onnx(\n","        num_labels: int = 2,\n","        model_weight_pth_path: str = \"./model_1.pth\",\n","        result_folder: str = \"./result_1\",\n","        pretrained_transformers_path: str = \"../hfl/chinese-roberta-wwm-ext\"\n","):\n","    if os.path.exists(result_folder):\n","        os.rmdir(result_folder)\n","    os.mkdir(result_folder)\n","\n","    # Default\n","    hugging_face_trained_model_path = os.path.join(result_folder, \"hugging_face_trained_model_path\")\n","    onnx_folder = os.path.join(result_folder, \"onnx\")\n","    unoptimized_onnx_path = os.path.join(onnx_folder, \"model.unoptimized.onnx\")\n","    optimized_onnx_path = os.path.join(onnx_folder, \"model.optimized.onnx\")\n","\n","    # step 1: save pth to hugging_face pipeline format\n","    print(\"step 1: save pth to hugging_face pipeline format\")\n","    model = BertForSequenceClassification.from_pretrained(pretrained_transformers_path, num_labels=num_labels)\n","    model.load_state_dict(torch.load(model_weight_pth_path))\n","    model.eval()\n","    model.save_pretrained(hugging_face_trained_model_path)\n","\n","    # step 2: export unoptimized onnx\n","    print(\"step 2: export unoptimized onnx\")\n","    os.mkdir(onnx_folder)\n","    convert(\n","        framework=\"pt\",\n","        model=hugging_face_trained_model_path,\n","        output=Path(unoptimized_onnx_path),\n","        tokenizer=pretrained_transformers_path,\n","        pipeline_name=\"sentiment-analysis\",\n","        opset=11)\n","\n","    # step 3: export optimized onnx\n","    print(\"step 3: export optimized onnx\")\n","    optimized_model = optimizer.optimize_model(unoptimized_onnx_path, use_gpu=True)\n","    optimized_model.save_model_to_file(optimized_onnx_path)\n","\n","    print(\"end export optimized onnx\")\n","    print(\"result onnx path:\", optimized_onnx_path)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJ2C3vb_si5m"},"source":["import numpy as np\n","from onnxruntime import InferenceSession\n","from transformers import BertTokenizerFast\n","from flask import Flask, request, jsonify\n","from threading import Lock\n","\n","\n","class TextPreprocessor:\n","    def __init__(self):\n","        self.lock = Lock()\n","        self.tokenizer = BertTokenizerFast.from_pretrained(\"./resource/hfl/chinese-roberta-wwm-ext\")\n","\n","    def preprocess(self, input_title: str, sample_paragraph):\n","        with self.lock:\n","            input_encoding = self.tokenizer(input_title, sample_paragraph,\n","                                            padding=\"max_length\",\n","                                            truncation=True,\n","                                            max_length=256)\n","        input_encoding = {k: np.expand_dims(np.asarray(v, dtype=np.int64), axis=0) for k, v in input_encoding.items()}\n","        return input_encoding\n","\n","\n","session = InferenceSession(\"./resource/model.summary.onnx\")\n","text_preprocessor = TextPreprocessor()\n","\n","\n","def calc_probability_level(title: str, paragraph: str) -> float:\n","    inputs_onnx = text_preprocessor.preprocess(title, paragraph)\n","    infer_result = session.run(None, inputs_onnx)[0][0]\n","    infer_result = np.power(np.e, infer_result)\n","\n","    probability = infer_result[1] / (np.sum(infer_result))\n","    print(\"---------\")\n","    print(paragraph)\n","    print(probability)\n","    return probability\n"],"execution_count":null,"outputs":[]}]}