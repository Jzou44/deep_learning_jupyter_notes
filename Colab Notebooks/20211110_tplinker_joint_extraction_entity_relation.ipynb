{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02ccff8e-e912-4a9a-a9d9-9a59c07f474e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load train dataset size: 18606\n",
      "load test dataset size: 2068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 0: 100%|██████████| 1163/1163 [03:56<00:00,  4.91it/s, loss=0.000184903, entity_precision=0.218397290, entity_recall=0.078245046, relation_precision=0.002915452, relation_recall=0.000396197]\n",
      "test epoch 0: 100%|██████████| 130/130 [00:18<00:00,  6.95it/s, loss=0.000179820, entity_precision=0.290644867, entity_recall=0.098416115, relation_precision=0.001675042, relation_recall=0.000100523]\n",
      "train epoch 1: 100%|██████████| 1163/1163 [03:54<00:00,  4.95it/s, loss=0.002000586, entity_precision=0.392136023, entity_recall=0.145018667, relation_precision=0.066568046, relation_recall=0.005888511]\n",
      "test epoch 1: 100%|██████████| 130/130 [00:18<00:00,  7.03it/s, loss=0.000114324, entity_precision=0.387666324, entity_recall=0.232969398, relation_precision=0.178649236, relation_recall=0.016485726]\n",
      "train epoch 2: 100%|██████████| 1163/1163 [03:55<00:00,  4.93it/s, loss=0.001969838, entity_precision=0.492180710, entity_recall=0.216966679, relation_precision=0.309090907, relation_recall=0.045822102]\n",
      "test epoch 2: 100%|██████████| 130/130 [00:17<00:00,  7.41it/s, loss=0.000077171, entity_precision=0.523706895, entity_recall=0.298938951, relation_precision=0.359283767, relation_recall=0.092782469]\n",
      "train epoch 3: 100%|██████████| 1163/1163 [03:52<00:00,  5.01it/s, loss=0.000061986, entity_precision=0.581967211, entity_recall=0.329101168, relation_precision=0.420382164, relation_recall=0.113372093]\n",
      "test epoch 3: 100%|██████████| 130/130 [00:17<00:00,  7.39it/s, loss=0.000057558, entity_precision=0.588933690, entity_recall=0.404274949, relation_precision=0.401408450, relation_recall=0.223462002]\n",
      "train epoch 4: 100%|██████████| 1163/1163 [03:52<00:00,  5.00it/s, loss=0.000050541, entity_precision=0.656391951, entity_recall=0.410928295, relation_precision=0.486101693, relation_recall=0.182326764]\n",
      "test epoch 4: 100%|██████████| 130/130 [00:17<00:00,  7.26it/s, loss=0.000044976, entity_precision=0.627056352, entity_recall=0.550976472, relation_precision=0.499583679, relation_recall=0.301568154]\n",
      "train epoch 5: 100%|██████████| 1163/1163 [03:55<00:00,  4.94it/s, loss=0.000041827, entity_precision=0.686613725, entity_recall=0.491773308, relation_precision=0.563197585, relation_recall=0.247481442]\n",
      "test epoch 5: 100%|██████████| 130/130 [00:17<00:00,  7.38it/s, loss=0.000036775, entity_precision=0.702395522, entity_recall=0.617714900, relation_precision=0.545073923, relation_recall=0.418777643]\n",
      "train epoch 6: 100%|██████████| 1163/1163 [03:55<00:00,  4.94it/s, loss=0.001917110, entity_precision=0.732087226, entity_recall=0.543771692, relation_precision=0.580314135, relation_recall=0.331578317]\n",
      "test epoch 6: 100%|██████████| 130/130 [00:18<00:00,  7.07it/s, loss=0.000032963, entity_precision=0.702050472, entity_recall=0.684453328, relation_precision=0.587115665, relation_recall=0.443405709]\n",
      "train epoch 7: 100%|██████████| 1163/1163 [03:51<00:00,  5.02it/s, loss=0.000030700, entity_precision=0.747064577, entity_recall=0.621995925, relation_precision=0.648977815, relation_recall=0.400913610]\n",
      "test epoch 7: 100%|██████████| 130/130 [00:18<00:00,  7.15it/s, loss=0.000030781, entity_precision=0.758655461, entity_recall=0.694141165, relation_precision=0.581656218, relation_recall=0.531664656]\n",
      "train epoch 8: 100%|██████████| 1163/1163 [03:53<00:00,  4.99it/s, loss=0.000029573, entity_precision=0.774163129, entity_recall=0.649396874, relation_precision=0.615611735, relation_recall=0.424859908]\n",
      "test epoch 8: 100%|██████████| 130/130 [00:18<00:00,  7.18it/s, loss=0.000032045, entity_precision=0.624330178, entity_recall=0.842072888, relation_precision=0.585842045, relation_recall=0.467531162]\n",
      "train epoch 9: 100%|██████████| 1163/1163 [03:51<00:00,  5.02it/s, loss=0.000025348, entity_precision=0.785287355, entity_recall=0.692198580, relation_precision=0.673049644, relation_recall=0.502914679]\n",
      "test epoch 9: 100%|██████████| 130/130 [00:18<00:00,  6.99it/s, loss=0.000027324, entity_precision=0.737523104, entity_recall=0.797631861, relation_precision=0.557781201, relation_recall=0.655006031]\n",
      "train epoch 10: 100%|██████████| 1163/1163 [03:52<00:00,  5.01it/s, loss=0.000022665, entity_precision=0.798885171, entity_recall=0.719333466, relation_precision=0.703138252, relation_recall=0.541193367]\n",
      "test epoch 10: 100%|██████████| 130/130 [00:17<00:00,  7.36it/s, loss=0.000023919, entity_precision=0.758671154, entity_recall=0.797170535, relation_precision=0.659196905, relation_recall=0.582529151]\n",
      "train epoch 11: 100%|██████████| 1163/1163 [03:52<00:00,  5.01it/s, loss=0.001899524, entity_precision=0.811600428, entity_recall=0.721818875, relation_precision=0.733665307, relation_recall=0.595833333]\n",
      "test epoch 11: 100%|██████████| 130/130 [00:19<00:00,  6.84it/s, loss=0.000025064, entity_precision=0.751288659, entity_recall=0.806858372, relation_precision=0.634730538, relation_recall=0.660635303]\n",
      "train epoch 12: 100%|██████████| 1163/1163 [03:53<00:00,  4.98it/s, loss=0.000020270, entity_precision=0.816663095, entity_recall=0.758654993, relation_precision=0.727096965, relation_recall=0.610819589]\n",
      "test epoch 12: 100%|██████████| 130/130 [00:19<00:00,  6.75it/s, loss=0.000024029, entity_precision=0.746621621, entity_recall=0.815623557, relation_precision=0.640912088, relation_recall=0.683755528]\n",
      "train epoch 13: 100%|██████████| 1163/1163 [03:55<00:00,  4.94it/s, loss=0.001896198, entity_precision=0.838253637, entity_recall=0.772117961, relation_precision=0.750385207, relation_recall=0.646356177]\n",
      "test epoch 13: 100%|██████████| 130/130 [00:18<00:00,  7.17it/s, loss=0.000024293, entity_precision=0.782844243, entity_recall=0.799938489, relation_precision=0.636649513, relation_recall=0.671592279]\n",
      "train epoch 14: 100%|██████████| 1163/1163 [03:51<00:00,  5.01it/s, loss=0.000014511, entity_precision=0.861525166, entity_recall=0.815541921, relation_precision=0.798479086, relation_recall=0.710227272]\n",
      "test epoch 14: 100%|██████████| 130/130 [00:18<00:00,  7.05it/s, loss=0.000023694, entity_precision=0.762651460, entity_recall=0.822697215, relation_precision=0.651095027, relation_recall=0.696320868]\n",
      "train epoch 15: 100%|██████████| 1163/1163 [03:55<00:00,  4.95it/s, loss=0.000013520, entity_precision=0.867888747, entity_recall=0.831449332, relation_precision=0.821782177, relation_recall=0.739026836]\n",
      "test epoch 15: 100%|██████████| 130/130 [00:18<00:00,  7.10it/s, loss=0.000022681, entity_precision=0.787170709, entity_recall=0.813316929, relation_precision=0.681588520, relation_recall=0.678025733]\n",
      "train epoch 16: 100%|██████████| 1163/1163 [03:55<00:00,  4.93it/s, loss=0.000012236, entity_precision=0.874035452, entity_recall=0.853216611, relation_precision=0.833144153, relation_recall=0.770805985]\n",
      "test epoch 16: 100%|██████████| 130/130 [00:18<00:00,  6.92it/s, loss=0.000023858, entity_precision=0.768421052, entity_recall=0.819467937, relation_precision=0.671740378, relation_recall=0.705367912]\n",
      "train epoch 17: 100%|██████████| 1163/1163 [03:56<00:00,  4.92it/s, loss=0.001889679, entity_precision=0.875383669, entity_recall=0.833268405, relation_precision=0.838718509, relation_recall=0.780581039]\n",
      "test epoch 17: 100%|██████████| 130/130 [00:18<00:00,  7.08it/s, loss=0.000023675, entity_precision=0.781081888, entity_recall=0.803782868, relation_precision=0.677623878, relation_recall=0.698331322]\n",
      "train epoch 18: 100%|██████████| 1163/1163 [03:53<00:00,  4.97it/s, loss=0.000010143, entity_precision=0.892820405, entity_recall=0.874823623, relation_precision=0.847374494, relation_recall=0.793033616]\n",
      "test epoch 18: 100%|██████████| 130/130 [00:18<00:00,  7.21it/s, loss=0.000024496, entity_precision=0.786455245, entity_recall=0.816084883, relation_precision=0.694649539, relation_recall=0.690390027]\n",
      "train epoch 19: 100%|██████████| 1163/1163 [03:55<00:00,  4.95it/s, loss=0.000009395, entity_precision=0.902448814, entity_recall=0.889240505, relation_precision=0.872906659, relation_recall=0.833312677]\n",
      "test epoch 19: 100%|██████████| 130/130 [00:17<00:00,  7.33it/s, loss=0.000025760, entity_precision=0.773179523, entity_recall=0.824542518, relation_precision=0.661342442, relation_recall=0.720044229]\n",
      "train epoch 20: 100%|██████████| 1163/1163 [03:56<00:00,  4.93it/s, loss=0.000008330, entity_precision=0.912865615, entity_recall=0.900888169, relation_precision=0.879521423, relation_recall=0.840132889]\n",
      "test epoch 20: 100%|██████████| 130/130 [00:18<00:00,  7.09it/s, loss=0.000027542, entity_precision=0.755893639, entity_recall=0.848070120, relation_precision=0.651574452, relation_recall=0.736328910]\n",
      "train epoch 21: 100%|██████████| 1163/1163 [03:53<00:00,  4.98it/s, loss=0.000008032, entity_precision=0.918355183, entity_recall=0.910577111, relation_precision=0.884396796, relation_recall=0.857160740]\n",
      "test epoch 21: 100%|██████████| 130/130 [00:17<00:00,  7.25it/s, loss=0.000028142, entity_precision=0.775072463, entity_recall=0.822389665, relation_precision=0.692529623, relation_recall=0.675613188]\n",
      "train epoch 22: 100%|██████████| 1163/1163 [03:54<00:00,  4.96it/s, loss=0.000006964, entity_precision=0.929117584, entity_recall=0.920556919, relation_precision=0.889822063, relation_recall=0.859362110]\n",
      "test epoch 22: 100%|██████████| 130/130 [00:17<00:00,  7.34it/s, loss=0.000029395, entity_precision=0.777665413, entity_recall=0.827771796, relation_precision=0.668292682, relation_recall=0.716123843]\n",
      "train epoch 23: 100%|██████████| 1163/1163 [03:54<00:00,  4.96it/s, loss=0.000006138, entity_precision=0.938116774, entity_recall=0.927627565, relation_precision=0.901571545, relation_recall=0.879741726]\n",
      "test epoch 23: 100%|██████████| 130/130 [00:18<00:00,  6.93it/s, loss=0.000028527, entity_precision=0.784969909, entity_recall=0.822389665, relation_precision=0.698778003, relation_recall=0.689786891]\n",
      "train epoch 24: 100%|██████████| 1163/1163 [03:54<00:00,  4.96it/s, loss=0.000005629, entity_precision=0.938418191, entity_recall=0.939931464, relation_precision=0.912809471, relation_recall=0.892749045]\n",
      "test epoch 24: 100%|██████████| 130/130 [00:18<00:00,  6.96it/s, loss=0.000030487, entity_precision=0.786911528, entity_recall=0.830232199, relation_precision=0.688908485, relation_recall=0.706775230]\n",
      "train epoch 25: 100%|██████████| 1163/1163 [03:53<00:00,  4.99it/s, loss=0.000005020, entity_precision=0.951469994, entity_recall=0.949557876, relation_precision=0.919116650, relation_recall=0.898305084]\n",
      "test epoch 25: 100%|██████████| 130/130 [00:17<00:00,  7.31it/s, loss=0.000030814, entity_precision=0.781195839, entity_recall=0.831769951, relation_precision=0.694630872, relation_recall=0.707478890]\n",
      "train epoch 26: 100%|██████████| 1163/1163 [03:55<00:00,  4.93it/s, loss=0.000004779, entity_precision=0.950387911, entity_recall=0.945753757, relation_precision=0.927601809, relation_recall=0.911468549]\n",
      "test epoch 26: 100%|██████████| 130/130 [00:18<00:00,  6.90it/s, loss=0.000032236, entity_precision=0.778305279, entity_recall=0.831923726, relation_precision=0.666605114, relation_recall=0.725774024]\n",
      "train epoch 27: 100%|██████████| 1163/1163 [03:51<00:00,  5.03it/s, loss=0.000004419, entity_precision=0.955296011, entity_recall=0.953184647, relation_precision=0.926966291, relation_recall=0.914127423]\n",
      "test epoch 27: 100%|██████████| 130/130 [00:18<00:00,  7.13it/s, loss=0.000032552, entity_precision=0.789707844, entity_recall=0.818852836, relation_precision=0.679746835, relation_recall=0.701749095]\n",
      "train epoch 28: 100%|██████████| 1163/1163 [03:55<00:00,  4.94it/s, loss=0.000004009, entity_precision=0.960526314, entity_recall=0.956521737, relation_precision=0.941123187, relation_recall=0.928389072]\n",
      "test epoch 28: 100%|██████████| 130/130 [00:17<00:00,  7.57it/s, loss=0.000034308, entity_precision=0.797464532, entity_recall=0.812548053, relation_precision=0.694427619, relation_recall=0.691495777]\n",
      "train epoch 29: 100%|██████████| 1163/1163 [03:51<00:00,  5.03it/s, loss=0.000003189, entity_precision=0.964820055, entity_recall=0.965991901, relation_precision=0.949903924, relation_recall=0.936916203]\n",
      "test epoch 29: 100%|██████████| 130/130 [00:17<00:00,  7.40it/s, loss=0.000036394, entity_precision=0.767827062, entity_recall=0.841150237, relation_precision=0.672247684, relation_recall=0.722456775]\n",
      "train epoch 30: 100%|██████████| 1163/1163 [03:54<00:00,  4.96it/s, loss=0.000002959, entity_precision=0.972268906, entity_recall=0.967593559, relation_precision=0.949074698, relation_recall=0.941066165]\n",
      "test epoch 30: 100%|██████████| 130/130 [00:17<00:00,  7.31it/s, loss=0.000038572, entity_precision=0.777155171, entity_recall=0.831769951, relation_precision=0.688914910, relation_recall=0.709690389]\n",
      "train epoch 31: 100%|██████████| 1163/1163 [03:52<00:00,  4.99it/s, loss=0.000002809, entity_precision=0.972692071, entity_recall=0.969137054, relation_precision=0.956285635, relation_recall=0.945757249]\n",
      "test epoch 31: 100%|██████████| 130/130 [00:18<00:00,  7.10it/s, loss=0.000037259, entity_precision=0.796078430, entity_recall=0.811625402, relation_precision=0.689651760, relation_recall=0.700743867]\n",
      "train epoch 32: 100%|██████████| 1163/1163 [03:53<00:00,  4.98it/s, loss=0.000002930, entity_precision=0.969286723, entity_recall=0.967722411, relation_precision=0.959506828, relation_recall=0.952141057]\n",
      "test epoch 32: 100%|██████████| 130/130 [00:18<00:00,  7.00it/s, loss=0.000039615, entity_precision=0.779939471, entity_recall=0.832231277, relation_precision=0.679304746, relation_recall=0.715018093]\n",
      "train epoch 33: 100%|██████████| 1163/1163 [03:54<00:00,  4.96it/s, loss=0.000002524, entity_precision=0.973005639, entity_recall=0.973594031, relation_precision=0.959824979, relation_recall=0.953252567]\n",
      "test epoch 33: 100%|██████████| 130/130 [00:18<00:00,  6.97it/s, loss=0.000038904, entity_precision=0.782404691, entity_recall=0.820544363, relation_precision=0.706262170, relation_recall=0.692702050]\n",
      "train epoch 34: 100%|██████████| 1163/1163 [03:54<00:00,  4.95it/s, loss=0.000002382, entity_precision=0.976019183, entity_recall=0.977973566, relation_precision=0.960443241, relation_recall=0.957728381]\n",
      "test epoch 34: 100%|██████████| 130/130 [00:18<00:00,  7.12it/s, loss=0.000039973, entity_precision=0.795010519, entity_recall=0.813470705, relation_precision=0.703760630, relation_recall=0.690390027]\n",
      "train epoch 35:   1%|          | 10/1163 [00:06<12:25,  1.55it/s, loss=0.000002089, entity_precision=0.980039901, entity_recall=0.987927546, relation_precision=0.971583207, relation_recall=0.944736830]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14891/1043629626.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# torch.save(model.state_dict(), f\"./model_1/model_{epoch}.pth\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14891/1043629626.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, epoch, scaler, optimizer)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0meh_et_matrix_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msh_oh_matrix_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst_ot_matrix_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [4, 26, 126, 126]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0meh_et_matrix_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msh_oh_matrix_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst_ot_matrix_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minput_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mbce_loss_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meh_et_matrix_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meh_et_matrix_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mbce_loss_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msh_oh_matrix_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msh_oh_matrix_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14891/1043629626.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbert_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0meh_et_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meh_et_pointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# entity_head to entity_tail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0msh_oh_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh_oh_pointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# subject_head to object_head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m         )\n\u001b[1;32m   1008\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    588\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m                 )\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         )\n\u001b[1;32m    477\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         )\n\u001b[1;32m    409\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "batch_size = 16\n",
    "transformers_path = \"../hfl/chinese-roberta-wwm-ext\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, train_mode: bool, transformers_path, max_len=128):\n",
    "        self.train_mode = train_mode\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(transformers_path)\n",
    "        data_file_path = \"./data/train_data.json\" if self.train_mode else \"./data/test_data.json\"\n",
    "        with open(data_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.dataset = json.load(f)\n",
    "        self.dataset_length = len(self.dataset)\n",
    "        print(f\"load {'train' if self.train_mode else 'test'} dataset size: {self.dataset_length}\")\n",
    "        with open(\"./data/entity_types.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            self.entity_types = json.load(f)\n",
    "            self.num_entity = len(self.entity_types)\n",
    "            self.id2entity = {idx: x for idx, x in enumerate(self.entity_types)}\n",
    "            self.entity2id = {x: idx for idx, x in enumerate(self.entity_types)}\n",
    "        with open(\"./data/relation_types.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            self.relation_types = json.load(f)\n",
    "            self.num_relation = len(self.relation_types)\n",
    "            self.id2relation = {idx: x for idx, x in enumerate(self.relation_types)}\n",
    "            self.relation2id = {x: idx for idx, x in enumerate(self.relation_types)}\n",
    "\n",
    "    def _encode(self, sample):\n",
    "        input_text = sample[\"text\"]\n",
    "        input_text_tokens = ['[CLS]'] + list(input_text)\n",
    "        input_text_tokens = input_text_tokens[:self.max_len - 1] + ['[SEP]']\n",
    "        input_text_tokens = input_text_tokens + ['[PAD]' for _ in range(self.max_len - len(input_text_tokens))]\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(input_text_tokens)\n",
    "        attention_mask = [1 if token != '[PAD]' else 0 for token in input_text_tokens]\n",
    "        token_type_ids = [0 for token in input_text_tokens]\n",
    "\n",
    "        _input_encoding = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids\n",
    "        }\n",
    "        _input_encoding = {k: torch.tensor(v, dtype=torch.long) for k, v in _input_encoding.items()}\n",
    "        return _input_encoding\n",
    "\n",
    "    def _calc_target_matrix(self, sample):\n",
    "        eh_et_matrix = np.zeros([self.num_entity, self.max_len - 2, self.max_len - 2], dtype=int)  # entity_head to entity_tail\n",
    "        sh_oh_matrix = np.zeros([self.num_relation, self.max_len - 2, self.max_len - 2], dtype=int)  # subject_head to object_head\n",
    "        st_ot_matrix = np.zeros([self.num_relation, self.max_len - 2, self.max_len - 2], dtype=int)  # subject_tail to object_tail\n",
    "        input_text = sample[\"text\"]\n",
    "        cache_entity = dict()\n",
    "        for entity in sample[\"entity_list\"]:\n",
    "            entity_text = entity[\"text\"]\n",
    "            entity_type = entity[\"type\"]\n",
    "            entity_type_id = self.entity2id[entity_type]\n",
    "            spans = [re_result.span() for re_result in re.finditer(re.escape(entity_text), input_text)]\n",
    "            spans = [(x1, x2 - 1) for x1, x2 in spans]\n",
    "            cache_entity[entity_text] = spans\n",
    "            for (x1, x2) in spans:\n",
    "                if x1 < self.max_len - 3 and x2 < self.max_len - 3:\n",
    "                    eh_et_matrix[entity_type_id, x1, x2] = 1\n",
    "\n",
    "        for relation in sample[\"relation_list\"]:\n",
    "            subject_text = relation[\"subject\"]\n",
    "            object_text = relation[\"object\"]\n",
    "            relation_type = relation[\"predicate\"]\n",
    "            relation_type_id = self.relation2id[relation_type]\n",
    "            for (sx1, sx2) in cache_entity[subject_text]:\n",
    "                for (ox1, ox2) in cache_entity[object_text]:\n",
    "                    if sx2 < self.max_len - 3 and ox2 < self.max_len - 3:\n",
    "                        sh_oh_matrix[relation_type_id, sx1, ox1] = 1\n",
    "                        st_ot_matrix[relation_type_id, sx2, ox2] = 1\n",
    "        return eh_et_matrix, sh_oh_matrix, st_ot_matrix\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        input_encoding = self._encode(sample)\n",
    "        eh_et_matrix, sh_oh_matrix, st_ot_matrix = self._calc_target_matrix(sample)\n",
    "        return input_encoding, eh_et_matrix, sh_oh_matrix, st_ot_matrix\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "\n",
    "class GlobalPointer(nn.Module):\n",
    "    def __init__(self, num_class, d_model):\n",
    "        super(GlobalPointer, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.proj_q = nn.Linear(d_model, num_class * d_model)\n",
    "        self.proj_k = nn.Linear(d_model, num_class * d_model)\n",
    "\n",
    "    def forward(self, embedding: torch.Tensor, mask, mask_tri: bool):\n",
    "        # embedding shape [batch, seq_len, d_model]\n",
    "        (batch, seq_len, d_model) = embedding.shape\n",
    "        q = self.proj_q(embedding).reshape([batch, seq_len, self.num_class, d_model])\n",
    "        k = self.proj_k(embedding).reshape([batch, seq_len, self.num_class, d_model])\n",
    "        tag_matrix = torch.einsum(\"bmcd,bncd->bcmn\", q, k)\n",
    "\n",
    "        mask_seq = torch.einsum(\"bm,bn->bmn\", mask, mask).unsqueeze(dim=-3)\n",
    "        if mask_tri:\n",
    "            mask_tri = torch.triu(torch.ones_like(tag_matrix))\n",
    "            mask = torch.logical_and(mask_seq, mask_tri)\n",
    "        else:\n",
    "            mask = mask_seq\n",
    "        tag_matrix = torch.masked_fill(tag_matrix, torch.logical_not(mask), -1e4)\n",
    "        return tag_matrix\n",
    "\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, transformers_path, num_entity, num_relation):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.bert_module = BertModel.from_pretrained(transformers_path)\n",
    "        self.eh_et_pointer = GlobalPointer(num_class=num_entity, d_model=self.bert_module.config.hidden_size)\n",
    "        self.sh_oh_pointer = GlobalPointer(num_class=num_relation, d_model=self.bert_module.config.hidden_size)\n",
    "        self.st_ot_pointer = GlobalPointer(num_class=num_relation, d_model=self.bert_module.config.hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        bert_output = self.bert_module(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)[0]\n",
    "        eh_et_matrix = self.eh_et_pointer(bert_output, attention_mask, True)  # entity_head to entity_tail\n",
    "        sh_oh_matrix = self.sh_oh_pointer(bert_output, attention_mask, False)  # subject_head to object_head\n",
    "        st_ot_matrix = self.st_ot_pointer(bert_output, attention_mask, False)  # subject_tail to object_tail\n",
    "        eh_et_matrix = eh_et_matrix[:, :, 1:-1, 1:-1]\n",
    "        sh_oh_matrix = sh_oh_matrix[:, :, 1:-1, 1:-1]\n",
    "        st_ot_matrix = st_ot_matrix[:, :, 1:-1, 1:-1]\n",
    "        return eh_et_matrix, sh_oh_matrix, st_ot_matrix\n",
    "\n",
    "\n",
    "def train(model, dataloader, epoch, scaler, optimizer):\n",
    "    time.sleep(0.2)\n",
    "    model.train()\n",
    "    loss_count = deque([], maxlen=100)\n",
    "    entity_tp_count = deque([], maxlen=100)\n",
    "    entity_fp_count = deque([], maxlen=100)\n",
    "    entity_fn_count = deque([], maxlen=100)\n",
    "    relation_tp_count = deque([], maxlen=100)\n",
    "    relation_fp_count = deque([], maxlen=100)\n",
    "    relation_fn_count = deque([], maxlen=100)\n",
    "    pbar = tqdm(dataloader)\n",
    "    pbar.set_description(\"train epoch {}\".format(epoch))\n",
    "    for input_encoding, eh_et_matrix_target, sh_oh_matrix_target, st_ot_matrix_target in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        input_encoding = {k: v.to(device) for k, v in input_encoding.items()}  # [4, 128]\n",
    "        eh_et_matrix_target, sh_oh_matrix_target, st_ot_matrix_target = \\\n",
    "            eh_et_matrix_target.to(device), sh_oh_matrix_target.to(device), st_ot_matrix_target.to(device)  # [4, 26, 126, 126]\n",
    "        with torch.cuda.amp.autocast():\n",
    "            eh_et_matrix_predict, sh_oh_matrix_predict, st_ot_matrix_predict = model(**input_encoding)\n",
    "            bce_loss_1 = F.binary_cross_entropy_with_logits(eh_et_matrix_predict, eh_et_matrix_target.float())\n",
    "            bce_loss_2 = F.binary_cross_entropy_with_logits(sh_oh_matrix_predict, sh_oh_matrix_target.float())\n",
    "            bce_loss_3 = F.binary_cross_entropy_with_logits(st_ot_matrix_predict, st_ot_matrix_target.float())\n",
    "            loss = bce_loss_1 + bce_loss_2 + bce_loss_3\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        loss_count.append(loss.item())\n",
    "        log_loss = np.mean(loss_count)\n",
    "\n",
    "        eh_et_matrix_predict = torch.gt(eh_et_matrix_predict, 0)\n",
    "        eh_et_matrix_target = torch.eq(eh_et_matrix_target, 1)\n",
    "        entity_tp_count.append(torch.logical_and(eh_et_matrix_predict, eh_et_matrix_target).sum().item())\n",
    "        entity_fp_count.append(torch.logical_and(eh_et_matrix_predict, torch.logical_not(eh_et_matrix_target)).sum().item())\n",
    "        entity_fn_count.append(torch.logical_and(torch.logical_not(eh_et_matrix_predict), eh_et_matrix_target).sum().item())\n",
    "\n",
    "        log_entity_tp, log_entity_fp, log_entity_fn = np.sum(entity_tp_count), np.sum(entity_fp_count), np.sum(entity_fn_count)\n",
    "        log_entity_precision = log_entity_tp / (log_entity_tp + log_entity_fp + 1e-5)\n",
    "        log_entity_recall = log_entity_tp / (log_entity_tp + log_entity_fn + 1e-5)\n",
    "\n",
    "        sh_oh_matrix_predict = torch.gt(sh_oh_matrix_predict, 0)\n",
    "        st_ot_matrix_predict = torch.gt(st_ot_matrix_predict, 0)\n",
    "        sh_oh_matrix_target = torch.eq(sh_oh_matrix_target, 1)\n",
    "        st_ot_matrix_target = torch.eq(st_ot_matrix_target, 1)\n",
    "        relation_tp_count.append(torch.logical_and(sh_oh_matrix_predict, sh_oh_matrix_target).sum().item() +\n",
    "                                 torch.logical_and(st_ot_matrix_predict, st_ot_matrix_target).sum().item())\n",
    "        relation_fp_count.append(torch.logical_and(sh_oh_matrix_predict, torch.logical_not(sh_oh_matrix_target)).sum().item() +\n",
    "                                 torch.logical_and(st_ot_matrix_predict, torch.logical_not(st_ot_matrix_target)).sum().item())\n",
    "        relation_fn_count.append(torch.logical_and(torch.logical_not(sh_oh_matrix_predict), sh_oh_matrix_target).sum().item()+\n",
    "                                 torch.logical_and(torch.logical_not(st_ot_matrix_predict), st_ot_matrix_target).sum().item())\n",
    "        log_relation_tp, log_relation_fp, log_relation_fn = np.sum(relation_tp_count), np.sum(relation_fp_count), np.sum(relation_fn_count)\n",
    "        log_relation_precision = log_relation_tp / (log_relation_tp + log_relation_fp + 1e-5)\n",
    "        log_relation_recall = log_relation_tp / (log_relation_tp + log_relation_fn + 1e-5)\n",
    "\n",
    "        log_str = f\"loss={log_loss:0.9f}, entity_precision={log_entity_precision:0.9f}, entity_recall={log_entity_recall:0.9f}, \" \\\n",
    "                  f\"relation_precision={log_relation_precision:0.9f}, relation_recall={log_relation_recall:0.9f}\"\n",
    "        pbar.set_postfix_str(log_str)\n",
    "\n",
    "\n",
    "def test(model, dataloader, epoch):\n",
    "    time.sleep(0.2)\n",
    "    model.eval()\n",
    "    loss_count = []\n",
    "    entity_tp_count = []\n",
    "    entity_fp_count = []\n",
    "    entity_fn_count = []\n",
    "    relation_tp_count = []\n",
    "    relation_fp_count = []\n",
    "    relation_fn_count = []\n",
    "    pbar = tqdm(dataloader)\n",
    "    pbar.set_description(\"test epoch {}\".format(epoch))\n",
    "    for input_encoding, eh_et_matrix_target, sh_oh_matrix_target, st_ot_matrix_target in pbar:\n",
    "        input_encoding = {k: v.to(device) for k, v in input_encoding.items()}  # [4, 128]\n",
    "        eh_et_matrix_target, sh_oh_matrix_target, st_ot_matrix_target = \\\n",
    "            eh_et_matrix_target.to(device), sh_oh_matrix_target.to(device), st_ot_matrix_target.to(device)  # [4, 26, 126, 126]\n",
    "        with torch.cuda.amp.autocast():\n",
    "            eh_et_matrix_predict, sh_oh_matrix_predict, st_ot_matrix_predict = model(**input_encoding)\n",
    "            bce_loss_1 = F.binary_cross_entropy_with_logits(eh_et_matrix_predict, eh_et_matrix_target.float())\n",
    "            bce_loss_2 = F.binary_cross_entropy_with_logits(sh_oh_matrix_predict, sh_oh_matrix_target.float())\n",
    "            bce_loss_3 = F.binary_cross_entropy_with_logits(st_ot_matrix_predict, st_ot_matrix_target.float())\n",
    "            loss = bce_loss_1 + bce_loss_2 + bce_loss_3\n",
    "        loss_count.append(loss.item())\n",
    "        log_loss = np.mean(loss_count)\n",
    "\n",
    "        eh_et_matrix_predict = torch.gt(eh_et_matrix_predict, 0)\n",
    "        eh_et_matrix_target = torch.eq(eh_et_matrix_target, 1)\n",
    "        entity_tp_count.append(torch.logical_and(eh_et_matrix_predict, eh_et_matrix_target).sum().item())\n",
    "        entity_fp_count.append(torch.logical_and(eh_et_matrix_predict, torch.logical_not(eh_et_matrix_target)).sum().item())\n",
    "        entity_fn_count.append(torch.logical_and(torch.logical_not(eh_et_matrix_predict), eh_et_matrix_target).sum().item())\n",
    "\n",
    "        log_entity_tp, log_entity_fp, log_entity_fn = np.sum(entity_tp_count), np.sum(entity_fp_count), np.sum(entity_fn_count)\n",
    "        log_entity_precision = log_entity_tp / (log_entity_tp + log_entity_fp + 1e-5)\n",
    "        log_entity_recall = log_entity_tp / (log_entity_tp + log_entity_fn + 1e-5)\n",
    "\n",
    "        sh_oh_matrix_predict = torch.gt(sh_oh_matrix_predict, 0)\n",
    "        st_ot_matrix_predict = torch.gt(st_ot_matrix_predict, 0)\n",
    "        sh_oh_matrix_target = torch.eq(sh_oh_matrix_target, 1)\n",
    "        st_ot_matrix_target = torch.eq(st_ot_matrix_target, 1)\n",
    "        relation_tp_count.append(torch.logical_and(sh_oh_matrix_predict, sh_oh_matrix_target).sum().item() +\n",
    "                                 torch.logical_and(st_ot_matrix_predict, st_ot_matrix_target).sum().item())\n",
    "        relation_fp_count.append(torch.logical_and(sh_oh_matrix_predict, torch.logical_not(sh_oh_matrix_target)).sum().item() +\n",
    "                                 torch.logical_and(st_ot_matrix_predict, torch.logical_not(st_ot_matrix_target)).sum().item())\n",
    "        relation_fn_count.append(torch.logical_and(torch.logical_not(sh_oh_matrix_predict), sh_oh_matrix_target).sum().item() +\n",
    "                                 torch.logical_and(torch.logical_not(st_ot_matrix_predict), st_ot_matrix_target).sum().item())\n",
    "        log_relation_tp, log_relation_fp, log_relation_fn = np.sum(relation_tp_count), np.sum(relation_fp_count), np.sum(relation_fn_count)\n",
    "        log_relation_precision = log_relation_tp / (log_relation_tp + log_relation_fp + 1e-5)\n",
    "        log_relation_recall = log_relation_tp / (log_relation_tp + log_relation_fn + 1e-5)\n",
    "\n",
    "        log_str = f\"loss={log_loss:0.9f}, entity_precision={log_entity_precision:0.9f}, entity_recall={log_entity_recall:0.9f}, \" \\\n",
    "                  f\"relation_precision={log_relation_precision:0.9f}, relation_recall={log_relation_recall:0.9f}\"\n",
    "        pbar.set_postfix_str(log_str)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset_train = CustomDataset(train_mode=True, transformers_path=transformers_path)\n",
    "    dataset_test = CustomDataset(train_mode=False, transformers_path=transformers_path)\n",
    "    # dataloader_train = DataLoader(dataset=dataset_train, batch_size=2, shuffle=True)\n",
    "    # dataloader_test = DataLoader(dataset=dataset_test, batch_size=2, shuffle=False)\n",
    "    dataloader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
    "    dataloader_test = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "    model = CustomModel(transformers_path, dataset_train.num_entity, dataset_train.num_relation)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(100):\n",
    "        train(model, dataloader_train, epoch, scaler, optimizer)\n",
    "        test(model, dataloader_test, epoch)\n",
    "        # torch.save(model.state_dict(), f\"./model_1/model_{epoch}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9105cd2-99fa-48eb-95e1-ee10051c821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"id\": 754,\n",
    "  \"text\": \"2006年主演《士兵突击》引起空前的反响，这让王宝强一下子成为了家喻户晓的明星\",\n",
    "  \"relation_list\": [\n",
    "    {\n",
    "      \"object\": \"王宝强\",\n",
    "      \"subject\": \"士兵突击\",\n",
    "      \"predicate\": \"主演\"\n",
    "    }\n",
    "  ],\n",
    "  \"entity_list\": [\n",
    "    {\n",
    "      \"text\": \"王宝强\",\n",
    "      \"type\": \"人物\"\n",
    "    },\n",
    "    {\n",
    "      \"text\": \"士兵突击\",\n",
    "      \"type\": \"影视作品\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
